#!/usr/bin/env python3
"""
è°ƒè¯•pipelineæ‰§è¡Œé¡ºåºé—®é¢˜
"""

import re
from typing import Dict, List, Set, Any

def analyze_pipeline_dependencies():
    """åˆ†æpipelineä¾èµ–å…³ç³»é—®é¢˜"""
    print("ğŸ” åˆ†æPipelineæ‰§è¡Œé¡ºåºé—®é¢˜")
    print("=" * 60)
    
    # ä»æ—¥å¿—ä¸­æå–çš„pipelineç»„ä»¶
    components = [
        {
            "id": "search_node",
            "tool_type": "smart_search",
            "params": {
                "query": "å¸¸å·å¤©ç›®æ¹–æ™¯åŒºæ—…æ¸¸ä¿¡æ¯",
                "max_results": 3
            }
        },
        {
            "id": "report_node",
            "tool_type": "enhanced_report_generator",
            "params": {
                "content": "$search_node.output.data.primary",
                "format": "markdown",
                "title": "å¸¸å·å¤©ç›®æ¹–æ™¯åŒºæ—…æ¸¸è·¯çº¿",
                "topic": "å¸¸å·å¤©ç›®æ¹–æ—…æ¸¸",
                "style": "professional"
            }
        },
        {
            "id": "file_writer_node",
            "tool_type": "file_writer",
            "params": {
                "file_path": "tianmu_lake_tour_guide.md",
                "text": "$enhanced_report_node.output.data.primary"
            }
        },
        {
            "id": "upload_node",
            "tool_type": "minio_uploader",
            "params": {
                "bucket_name": "kb-dev",
                "file_path": "$file_writer_node.output.data.primary"
            }
        }
    ]
    
    print("ğŸ“‹ Pipelineç»„ä»¶åˆ†æ:")
    for i, comp in enumerate(components):
        print(f"\nç»„ä»¶ {i+1}: {comp['id']}")
        print(f"  å·¥å…·ç±»å‹: {comp['tool_type']}")
        print(f"  å‚æ•°: {comp['params']}")
    
    # åˆ†æä¾èµ–å…³ç³»
    print(f"\nğŸ”— ä¾èµ–å…³ç³»åˆ†æ:")
    dependencies = {}
    node_ids = {comp["id"] for comp in components}
    
    for component in components:
        node_id = component["id"]
        dependencies[node_id] = set()
        
        # æ£€æŸ¥è¯¥èŠ‚ç‚¹ä¾èµ–çš„å…¶ä»–èŠ‚ç‚¹
        params = component.get("params", {})
        placeholder_refs = extract_placeholder_references(params)
        
        print(f"\n{node_id} çš„å ä½ç¬¦å¼•ç”¨:")
        for ref in placeholder_refs:
            print(f"  - å¼•ç”¨èŠ‚ç‚¹: {ref['node_id']}")
            print(f"    è¾“å‡ºé”®: {ref['output_key']}")
            if ref["node_id"] in node_ids:
                dependencies[node_id].add(ref["node_id"])
                print(f"    âœ“ ä¾èµ–å…³ç³»å»ºç«‹")
            else:
                print(f"    âœ— å¼•ç”¨çš„èŠ‚ç‚¹ä¸å­˜åœ¨")
    
    print(f"\nğŸ“Š ä¾èµ–å…³ç³»å›¾:")
    for node_id, deps in dependencies.items():
        if deps:
            print(f"  {node_id} ä¾èµ–: {list(deps)}")
        else:
            print(f"  {node_id} æ— ä¾èµ–")
    
    # æ„å»ºæ‰§è¡Œé¡ºåº
    print(f"\nğŸ“‹ æ„å»ºæ‰§è¡Œé¡ºåº:")
    execution_order = build_execution_order(components)
    print(f"å½“å‰æ‰§è¡Œé¡ºåº: {' -> '.join(execution_order)}")
    
    # åˆ†æé—®é¢˜
    print(f"\nğŸ” é—®é¢˜åˆ†æ:")
    print("ä»ä¾èµ–å…³ç³»å›¾å¯ä»¥çœ‹å‡º:")
    print("1. search_node: æ— ä¾èµ–")
    print("2. report_node: ä¾èµ– search_node")
    print("3. file_writer_node: ä¾èµ– enhanced_report_node (ä½†åº”è¯¥æ˜¯ report_node)")
    print("4. upload_node: ä¾èµ– file_writer_node")
    
    print(f"\nâŒ å‘ç°çš„é—®é¢˜:")
    print("1. file_writer_node å¼•ç”¨äº† 'enhanced_report_node' ä½†å®é™…èŠ‚ç‚¹IDæ˜¯ 'report_node'")
    print("2. è¿™å¯¼è‡´ file_writer_node è¢«è®¤ä¸ºæ²¡æœ‰ä¾èµ–ï¼Œå¯ä»¥æœ€å…ˆæ‰§è¡Œ")
    
    # ä¿®å¤å»ºè®®
    print(f"\nğŸ”§ ä¿®å¤æ–¹æ¡ˆ:")
    print("1. ä¿®æ­£èŠ‚ç‚¹IDå¼•ç”¨:")
    print("   - file_writer_node.params.text: '$report_node.output.data.primary'")
    
    print(f"\nâœ… æ­£ç¡®çš„æ‰§è¡Œé¡ºåºåº”è¯¥æ˜¯:")
    print("1. search_node (æœç´¢ä¿¡æ¯)")
    print("2. report_node (ç”ŸæˆæŠ¥å‘Š)")
    print("3. file_writer_node (å†™å…¥æ–‡ä»¶)")
    print("4. upload_node (ä¸Šä¼ æ–‡ä»¶)")

def extract_placeholder_references(params: Dict[str, Any]) -> List[Dict[str, str]]:
    """æå–å‚æ•°ä¸­çš„æ‰€æœ‰å ä½ç¬¦å¼•ç”¨"""
    references = []
    placeholder_pattern = r'\$([^.]+)\.output(?:\.([^.]+))?'
    
    def extract_from_value(value):
        if isinstance(value, str):
            matches = re.finditer(placeholder_pattern, value)
            for match in matches:
                references.append({
                    "node_id": match.group(1),
                    "output_key": match.group(2)
                })
        elif isinstance(value, dict):
            for v in value.values():
                extract_from_value(v)
        elif isinstance(value, list):
            for v in value:
                extract_from_value(v)
    
    extract_from_value(params)
    return references

def build_execution_order(components: List[Dict[str, Any]]) -> List[str]:
    """æ„å»ºpipelineçš„æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰"""
    # æ„å»ºä¾èµ–å›¾
    dependencies = {}
    node_ids = {comp["id"] for comp in components}
    
    for component in components:
        node_id = component["id"]
        dependencies[node_id] = set()
        
        # æ£€æŸ¥è¯¥èŠ‚ç‚¹ä¾èµ–çš„å…¶ä»–èŠ‚ç‚¹
        params = component.get("params", {})
        placeholder_refs = extract_placeholder_references(params)
        
        for ref in placeholder_refs:
            if ref["node_id"] in node_ids:
                dependencies[node_id].add(ref["node_id"])
    
    # æ‹“æ‰‘æ’åº
    execution_order = []
    visited = set()
    temp_visited = set()
    
    def visit(node_id):
        if node_id in temp_visited:
            raise ValueError(f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {node_id}")
        if node_id in visited:
            return
            
        temp_visited.add(node_id)
        
        for dep in dependencies.get(node_id, []):
            visit(dep)
            
        temp_visited.remove(node_id)
        visited.add(node_id)
        execution_order.append(node_id)
    
    for node_id in node_ids:
        if node_id not in visited:
            visit(node_id)
            
    return execution_order

if __name__ == "__main__":
    analyze_pipeline_dependencies() 