#!/usr/bin/env python3
"""
å·¥å…·é€‚é…å™¨ - å®ç°æ™ºèƒ½çš„å·¥å…·è‡ªé€‚åº”å’ŒåŠ¨æ€æ‰©å±•
è§£å†³pipelineè¾“å‡ºç»“æ„ä¸å·¥å…·è¾“å…¥æœŸæœ›ä¸åŒ¹é…çš„é—®é¢˜

è®¾è®¡åŸåˆ™ï¼š
1. é€šç”¨æ€§ï¼šä¸ç¡¬ç¼–ç ç‰¹å®šå­—æ®µåï¼Œæ”¯æŒä»»æ„å­—æ®µæ˜ å°„
2. å¯æ‰©å±•æ€§ï¼šæ”¯æŒè‡ªå®šä¹‰æ˜ å°„è§„åˆ™å’Œè½¬æ¢å‡½æ•°
3. æ™ºèƒ½æ€§ï¼šè‡ªåŠ¨å­¦ä¹ æ•°æ®ç»“æ„å’Œè½¬æ¢æ¨¡å¼
4. å®¹é”™æ€§ï¼šä¼˜é›…å¤„ç†è½¬æ¢å¤±è´¥çš„æƒ…å†µ
"""

import logging
import inspect
import asyncio
import time
from typing import Dict, Any, List, Optional, Callable, Union, Tuple, Type
from dataclasses import dataclass, field
from enum import Enum
import json
import re
from abc import ABC, abstractmethod

# é¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨ç±»å‹æ³¨è§£
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .placeholder_resolver import NodeOutput

class AdapterType(Enum):
    """é€‚é…å™¨ç±»å‹"""
    OUTPUT_MAPPER = "output_mapper"  # è¾“å‡ºæ˜ å°„é€‚é…å™¨
    INPUT_TRANSFORMER = "input_transformer"  # è¾“å…¥è½¬æ¢é€‚é…å™¨
    DATA_CONVERTER = "data_converter"  # æ•°æ®æ ¼å¼è½¬æ¢å™¨
    AUTO_GENERATED = "auto_generated"  # è‡ªåŠ¨ç”Ÿæˆçš„é€‚é…å™¨
    CUSTOM = "custom"  # è‡ªå®šä¹‰é€‚é…å™¨

@dataclass
class MappingRule:
    """æ˜ å°„è§„åˆ™å®šä¹‰"""
    source_pattern: str  # æºå­—æ®µæ¨¡å¼ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
    target_pattern: str  # ç›®æ ‡å­—æ®µæ¨¡å¼
    transformation: Optional[str] = None  # è½¬æ¢å‡½æ•°å
    condition: Optional[str] = None  # åº”ç”¨æ¡ä»¶
    priority: int = 1  # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰

@dataclass
class AdapterDefinition:
    """é€‚é…å™¨å®šä¹‰"""
    name: str
    adapter_type: AdapterType
    source_tool: str
    target_tool: str
    mapping_rules: List[MappingRule] = field(default_factory=list)
    code: str = ""
    is_active: bool = True
    created_at: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

class DataStructureAnalyzer:
    """æ•°æ®ç»“æ„åˆ†æå™¨"""
    
    @staticmethod
    def analyze_structure(data: Any, max_depth: int = 3) -> Dict[str, Any]:
        """
        åˆ†ææ•°æ®ç»“æ„
        
        Args:
            data: è¦åˆ†æçš„æ•°æ®
            max_depth: æœ€å¤§åˆ†ææ·±åº¦
            
        Returns:
            ç»“æ„åˆ†æç»“æœ
        """
        return DataStructureAnalyzer._analyze_recursive(data, max_depth, 0)
    
    @staticmethod
    def _analyze_recursive(data: Any, max_depth: int, current_depth: int) -> Dict[str, Any]:
        """é€’å½’åˆ†ææ•°æ®ç»“æ„"""
        if current_depth >= max_depth:
            return {"type": "max_depth_reached", "value": str(data)[:100]}
        
        if data is None:
            return {"type": "null"}
        
        if isinstance(data, (str, int, float, bool)):
            return {
                "type": type(data).__name__,
                "value": data,
                "length": len(str(data)) if isinstance(data, str) else None
            }
        
        if isinstance(data, list):
            if not data:
                return {"type": "list", "empty": True}
            
            # åˆ†æåˆ—è¡¨å…ƒç´ 
            element_analysis = DataStructureAnalyzer._analyze_recursive(
                data[0], max_depth, current_depth + 1
            )
            
            return {
                "type": "list",
                "length": len(data),
                "element_type": element_analysis["type"],
                "element_analysis": element_analysis,
                "uniform": all(
                    DataStructureAnalyzer._analyze_recursive(item, max_depth, current_depth + 1)["type"] 
                    == element_analysis["type"] 
                    for item in data[:5]  # åªæ£€æŸ¥å‰5ä¸ªå…ƒç´ 
                )
            }
        
        if isinstance(data, dict):
            structure = {
                "type": "dict",
                "keys": list(data.keys()),
                "key_count": len(data),
                "fields": {}
            }
            
            for key, value in data.items():
                structure["fields"][key] = DataStructureAnalyzer._analyze_recursive(
                    value, max_depth, current_depth + 1
                )
            
            return structure
        
        # å¤„ç†å…¶ä»–ç±»å‹ï¼ˆå¦‚PIL Imageç­‰ï¼‰
        result = {
            "type": type(data).__name__,
            "attributes": [attr for attr in dir(data) if not attr.startswith('_')][:10],
            "has_size": hasattr(data, 'size'),
            "has_shape": hasattr(data, 'shape'),
            "string_repr": str(data)[:100]
        }
        
        # ç‰¹æ®Šå¤„ç†PIL Image
        if hasattr(data, 'save') and hasattr(data, 'size'):
            result["type"] = "PIL_Image"
            result["image_info"] = {
                "size": data.size,
                "mode": getattr(data, 'mode', 'unknown'),
                "format": getattr(data, 'format', 'unknown')
            }
        
        return result

class MappingRuleEngine:
    """æ˜ å°„è§„åˆ™å¼•æ“"""
    
    def __init__(self):
        self.rules: List[MappingRule] = []
        self._load_default_rules()
    
    def _load_default_rules(self):
        """åŠ è½½é»˜è®¤æ˜ å°„è§„åˆ™"""
        # é€šç”¨ç±»å‹è½¬æ¢è§„åˆ™
        self.rules.extend([
            MappingRule(
                source_pattern="*",
                target_pattern="*",
                transformation="identity",
                priority=100
            ),
            MappingRule(
                source_pattern="*.list",
                target_pattern="*.array",
                transformation="list_to_array",
                priority=10
            ),
            MappingRule(
                source_pattern="*.string",
                target_pattern="*.number",
                transformation="string_to_number",
                priority=10
            ),
            MappingRule(
                source_pattern="*.number",
                target_pattern="*.string",
                transformation="number_to_string",
                priority=10
            )
        ])
    
    def add_rule(self, rule: MappingRule):
        """æ·»åŠ æ˜ å°„è§„åˆ™"""
        self.rules.append(rule)
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.rules.sort(key=lambda r: r.priority)
    
    def find_matching_rules(self, source_structure: Dict[str, Any], 
                          target_structure: Dict[str, Any]) -> List[MappingRule]:
        """æŸ¥æ‰¾åŒ¹é…çš„æ˜ å°„è§„åˆ™"""
        matching_rules = []
        
        for rule in self.rules:
            if self._rule_matches(rule, source_structure, target_structure):
                matching_rules.append(rule)
        
        return matching_rules
    
    def _rule_matches(self, rule: MappingRule, source_structure: Dict[str, Any], 
                     target_structure: Dict[str, Any]) -> bool:
        """æ£€æŸ¥è§„åˆ™æ˜¯å¦åŒ¹é…"""
        # ç®€åŒ–çš„æ¨¡å¼åŒ¹é…å®ç°
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ¨¡å¼åŒ¹é…é€»è¾‘
        return True  # æš‚æ—¶è¿”å›Trueï¼Œåç»­å¯ä»¥æ‰©å±•

class TransformationEngine:
    """è½¬æ¢å¼•æ“"""
    
    def __init__(self):
        self.transformers: Dict[str, Callable] = {}
        self._load_default_transformers()
    
    def _load_default_transformers(self):
        """åŠ è½½é»˜è®¤è½¬æ¢å™¨"""
        self.transformers.update({
            "identity": lambda x: x,
            "list_to_array": lambda x: x if isinstance(x, list) else [x],
            "array_to_list": lambda x: list(x) if hasattr(x, '__iter__') else [x],
            "string_to_number": lambda x: float(x) if isinstance(x, str) and x.replace('.', '').replace('-', '').isdigit() else x,
            "number_to_string": lambda x: str(x) if isinstance(x, (int, float)) else x,
            "dict_to_list": lambda x: list(x.values()) if isinstance(x, dict) else x,
            "list_to_dict": lambda x: {f"item_{i}": item for i, item in enumerate(x)} if isinstance(x, list) else x,
            "flatten_list": lambda x: [item for sublist in x for item in (sublist if isinstance(sublist, list) else [sublist])] if isinstance(x, list) else x,
            "wrap_single": lambda x: [x] if not isinstance(x, list) else x,
            "unwrap_single": lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x
        })
    
    def register_transformer(self, name: str, transformer: Callable):
        """æ³¨å†Œè½¬æ¢å™¨"""
        self.transformers[name] = transformer
    
    def apply_transformation(self, data: Any, transformation: str) -> Any:
        """åº”ç”¨è½¬æ¢"""
        if transformation not in self.transformers:
            logging.warning(f"æœªçŸ¥çš„è½¬æ¢å™¨: {transformation}")
            return data
        
        try:
            return self.transformers[transformation](data)
        except Exception as e:
            logging.error(f"åº”ç”¨è½¬æ¢å™¨ {transformation} å¤±è´¥: {e}")
            return data

class AdapterStatistics:
    """é€‚é…å™¨ç»Ÿè®¡ä¿¡æ¯"""
    
    def __init__(self):
        self.total_adaptations = 0
        self.successful_adaptations = 0
        self.failed_adaptations = 0
        self.adaptation_times = []
        self.adapter_usage = {}
        self.error_counts = {}
    
    def record_adaptation(self, adapter_name: str, success: bool, duration: float):
        """è®°å½•é€‚é…æ“ä½œ"""
        self.total_adaptations += 1
        if success:
            self.successful_adaptations += 1
        else:
            self.failed_adaptations += 1
        
        self.adaptation_times.append(duration)
        
        if adapter_name not in self.adapter_usage:
            self.adapter_usage[adapter_name] = {"success": 0, "failed": 0, "total_time": 0}
        
        self.adapter_usage[adapter_name]["success" if success else "failed"] += 1
        self.adapter_usage[adapter_name]["total_time"] += duration
    
    def record_error(self, error_type: str):
        """è®°å½•é”™è¯¯"""
        if error_type not in self.error_counts:
            self.error_counts[error_type] = 0
        self.error_counts[error_type] += 1
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        avg_time = sum(self.adaptation_times) / len(self.adaptation_times) if self.adaptation_times else 0
        
        return {
            "total_adaptations": self.total_adaptations,
            "successful_adaptations": self.successful_adaptations,
            "failed_adaptations": self.failed_adaptations,
            "success_rate": self.successful_adaptations / self.total_adaptations if self.total_adaptations > 0 else 0,
            "average_time": avg_time,
            "min_time": min(self.adaptation_times) if self.adaptation_times else 0,
            "max_time": max(self.adaptation_times) if self.adaptation_times else 0,
            "adapter_usage": self.adapter_usage,
            "error_counts": self.error_counts
        }

class AdapterConfiguration:
    """é€‚é…å™¨é…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.config = {
            "max_analysis_depth": 3,
            "similarity_threshold": 0.3,
            "enable_caching": True,
            "cache_size": 1000,
            "enable_parallel": False,
            "max_workers": 4,
            "timeout": 30.0,
            "retry_count": 3,
            "log_level": "INFO"
        }
    
    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®"""
        self.config.update(kwargs)
    
    def get_config(self, key: str, default=None):
        """è·å–é…ç½®å€¼"""
        return self.config.get(key, default)
    
    def load_from_file(self, file_path: str):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            import yaml
            with open(file_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
                self.config.update(config_data)
        except Exception as e:
            logging.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def save_to_file(self, file_path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            import yaml
            with open(file_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# æ‰©å±•ToolAdapterç±»
class ToolAdapter:
    """å·¥å…·é€‚é…å™¨ - æ™ºèƒ½é€‚é…å·¥å…·é—´çš„æ•°æ®æ ¼å¼ä¸åŒ¹é…"""
    
    def __init__(self):
        self.logger = logging.getLogger("ToolAdapter")
        self.adapters: Dict[str, AdapterDefinition] = {}
        self.adapter_cache: Dict[str, Callable] = {}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.structure_analyzer = DataStructureAnalyzer()
        self.rule_engine = MappingRuleEngine()
        self.transformation_engine = TransformationEngine()
        
        # æ–°å¢ç»„ä»¶
        self.statistics = AdapterStatistics()
        self.config = AdapterConfiguration()
        
        # ç¼“å­˜ç®¡ç†
        self._cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
    def analyze_compatibility(self, source_output: 'NodeOutput', target_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææºå·¥å…·è¾“å‡ºä¸ç›®æ ‡å·¥å…·è¾“å…¥çš„å…¼å®¹æ€§
        
        Args:
            source_output: æºå·¥å…·çš„è¾“å‡º
            target_params: ç›®æ ‡å·¥å…·çš„å‚æ•°å®šä¹‰
            
        Returns:
            å…¼å®¹æ€§åˆ†æç»“æœ
        """
        analysis = {
            "is_compatible": True,
            "missing_keys": [],
            "type_mismatches": [],
            "suggested_mappings": [],
            "confidence": 1.0,
            "source_structure": None,
            "target_structure": None
        }
        
        try:
            # åˆ†ææºè¾“å‡ºç»“æ„
            source_structure = self.structure_analyzer.analyze_structure(source_output.value)
            analysis["source_structure"] = source_structure
            
            # åˆ†æç›®æ ‡å‚æ•°ç»“æ„
            target_structure = self._analyze_target_structure(target_params)
            analysis["target_structure"] = target_structure
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ˜ å°„è§„åˆ™
            matching_rules = self.rule_engine.find_matching_rules(source_structure, target_structure)
            
            # ç”Ÿæˆæ˜ å°„å»ºè®®
            analysis["suggested_mappings"] = self._generate_mapping_suggestions(
                source_structure, target_structure, matching_rules
            )
            
            # æ£€æŸ¥å…¼å®¹æ€§é—®é¢˜
            compatibility_issues = self._check_compatibility_issues(
                source_structure, target_structure, analysis["suggested_mappings"]
            )
            
            analysis["missing_keys"] = compatibility_issues["missing_keys"]
            analysis["type_mismatches"] = compatibility_issues["type_mismatches"]
            analysis["is_compatible"] = len(compatibility_issues["missing_keys"]) == 0 and len(compatibility_issues["type_mismatches"]) == 0
            
            # è®¡ç®—ç½®ä¿¡åº¦
            analysis["confidence"] = self._calculate_compatibility_confidence(analysis)
            
        except Exception as e:
            self.logger.error(f"å…¼å®¹æ€§åˆ†æå¤±è´¥: {e}")
            analysis["is_compatible"] = False
            analysis["confidence"] = 0.0
        
        return analysis
    
    def create_adapter(self, source_tool: str, target_tool: str, 
                      source_output: 'NodeOutput', target_params: Dict[str, Any]) -> Optional[AdapterDefinition]:
        """
        åˆ›å»ºé€‚é…å™¨æ¥è§£å†³å…¼å®¹æ€§é—®é¢˜
        
        Args:
            source_tool: æºå·¥å…·åç§°
            target_tool: ç›®æ ‡å·¥å…·åç§°
            source_output: æºå·¥å…·è¾“å‡º
            target_params: ç›®æ ‡å·¥å…·å‚æ•°
            
        Returns:
            åˆ›å»ºçš„é€‚é…å™¨å®šä¹‰
        """
        try:
            # åˆ†æå…¼å®¹æ€§
            analysis = self.analyze_compatibility(source_output, target_params)
            
            if analysis["is_compatible"]:
                self.logger.info(f"å·¥å…· {source_tool} å’Œ {target_tool} å…¼å®¹ï¼Œæ— éœ€é€‚é…å™¨")
                return None
            
            # ç”Ÿæˆé€‚é…å™¨ä»£ç 
            adapter_code = self._generate_adaptive_adapter_code(
                source_tool, target_tool, source_output, target_params, analysis
            )
            
            # åˆ›å»ºé€‚é…å™¨å®šä¹‰
            adapter_name = f"{source_tool}_to_{target_tool}_adapter"
            adapter_def = AdapterDefinition(
                name=adapter_name,
                adapter_type=AdapterType.AUTO_GENERATED,
                source_tool=source_tool,
                target_tool=target_tool,
                mapping_rules=[MappingRule(
                    source_pattern="*",
                    target_pattern="*",
                    transformation="adaptive"
                )],
                code=adapter_code,
                metadata={"analysis": analysis}
            )
            
            # æ³¨å†Œé€‚é…å™¨
            self.adapters[adapter_name] = adapter_def
            
            # ç¼–è¯‘å¹¶ç¼“å­˜é€‚é…å™¨
            adapter_func = self._compile_adapter(adapter_name, adapter_code)
            if adapter_func:
                self.adapter_cache[adapter_name] = adapter_func
                self.logger.info(f"æˆåŠŸåˆ›å»ºé€‚é…å™¨: {adapter_name}")
                return adapter_def
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºé€‚é…å™¨å¤±è´¥: {e}")
        
        return None
    
    def apply_adapter(self, adapter_name: str, source_data: Any) -> Any:
        """
        åº”ç”¨é€‚é…å™¨è½¬æ¢æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            adapter_name: é€‚é…å™¨åç§°
            source_data: æºæ•°æ®
            
        Returns:
            è½¬æ¢åçš„æ•°æ®
        """
        if adapter_name not in self.adapter_cache:
            self.logger.error(f"é€‚é…å™¨ {adapter_name} æœªæ‰¾åˆ°")
            self.statistics.record_error("adapter_not_found")
            return source_data
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{adapter_name}:{hash(str(source_data))}"
        if self.config.get_config("enable_caching") and cache_key in self._cache:
            self._cache_hits += 1
            return self._cache[cache_key]
        
        self._cache_misses += 1
        start_time = time.time()
        
        try:
            adapter_func = self.adapter_cache[adapter_name]
            result = adapter_func(source_data)
            
            # è®°å½•æˆåŠŸ
            duration = time.time() - start_time
            self.statistics.record_adaptation(adapter_name, True, duration)
            
            # ç¼“å­˜ç»“æœ
            if self.config.get_config("enable_caching"):
                if len(self._cache) >= self.config.get_config("cache_size"):
                    # ç®€å•çš„LRUç¼“å­˜ï¼šåˆ é™¤æœ€æ—§çš„æ¡ç›®
                    oldest_key = next(iter(self._cache))
                    del self._cache[oldest_key]
                self._cache[cache_key] = result
            
            self.logger.info(f"é€‚é…å™¨ {adapter_name} åº”ç”¨æˆåŠŸï¼Œè€—æ—¶: {duration:.4f}ç§’")
            return result
            
        except Exception as e:
            # è®°å½•å¤±è´¥
            duration = time.time() - start_time
            self.statistics.record_adaptation(adapter_name, False, duration)
            self.statistics.record_error("adapter_execution_failed")
            
            self.logger.error(f"åº”ç”¨é€‚é…å™¨ {adapter_name} å¤±è´¥: {e}")
            return source_data
    
    def auto_adapt_output(self, source_output: 'NodeOutput', target_expectation: Dict[str, Any]) -> Any:
        """
        è‡ªåŠ¨é€‚é…è¾“å‡ºä»¥åŒ¹é…ç›®æ ‡æœŸæœ›
        
        Args:
            source_output: æºå·¥å…·è¾“å‡º
            target_expectation: ç›®æ ‡æœŸæœ›çš„ç»“æ„
            
        Returns:
            é€‚é…åçš„è¾“å‡º
        """
        try:
            self.logger.info(f"ğŸ”„ è‡ªåŠ¨é€‚é…è¾“å‡ºå¼€å§‹:")
            self.logger.info(f"   æºèŠ‚ç‚¹: {source_output.node_id}")
            self.logger.info(f"   æºè¾“å‡ºé”®: {source_output.output_key}")
            self.logger.info(f"   æºè¾“å‡ºå€¼ç±»å‹: {type(source_output.value)}")
            self.logger.info(f"   æºè¾“å‡ºå€¼: {source_output.value}")
            self.logger.info(f"   ç›®æ ‡æœŸæœ›: {target_expectation}")
            
            # åˆ†ææºæ•°æ®ç»“æ„
            source_structure = self.structure_analyzer.analyze_structure(source_output.value)
            self.logger.info(f"   åˆ†æå¾—åˆ°çš„æºç»“æ„: {source_structure}")
            
            # å°è¯•æ™ºèƒ½æ˜ å°„
            self.logger.info("   å°è¯•æ™ºèƒ½æ˜ å°„...")
            adapted_output = self._intelligent_mapping(source_output.value, source_structure, target_expectation)
            
            if adapted_output is not None:
                self.logger.info(f"âœ… æ™ºèƒ½æ˜ å°„æˆåŠŸï¼Œè¿”å›: {adapted_output}")
                return adapted_output
            
            self.logger.warning("âŒ æ™ºèƒ½æ˜ å°„å¤±è´¥ï¼Œå°è¯•åˆ›å»ºé€‚é…å™¨...")
            
            # å¦‚æœæ™ºèƒ½æ˜ å°„å¤±è´¥ï¼Œå°è¯•åˆ›å»ºé€‚é…å™¨
            adapter_def = self.create_adapter(
                source_output.node_id, 
                "target_tool",  # è¿™é‡Œéœ€è¦ä»ä¸Šä¸‹æ–‡è·å–
                source_output, 
                target_expectation
            )
            
            if adapter_def:
                self.logger.info(f"   åˆ›å»ºé€‚é…å™¨æˆåŠŸ: {adapter_def.name}")
                return self.apply_adapter(adapter_def.name, source_output.value)
            
            # æœ€åçš„fallbackï¼šè¿”å›åŸå§‹æ•°æ®
            self.logger.warning("âš ï¸  æ— æ³•é€‚é…è¾“å‡ºï¼Œè¿”å›åŸå§‹æ•°æ®")
            return source_output.value
            
        except Exception as e:
            self.logger.error(f"âŒ è‡ªåŠ¨é€‚é…è¾“å‡ºå¤±è´¥: {e}")
            return source_output.value
    
    def _analyze_target_structure(self, target_params: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç›®æ ‡å‚æ•°ç»“æ„"""
        structure = {
            "type": "dict",
            "required_keys": [],
            "optional_keys": [],
            "type_requirements": {},
            "patterns": {}
        }
        
        for key, value in target_params.items():
            if isinstance(value, str) and value.startswith('$'):
                # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å¼•ç”¨
                structure["required_keys"].append(key)
                # å°è¯•æ¨æ–­ç±»å‹
                if "image" in key.lower() or "path" in key.lower():
                    structure["type_requirements"][key] = "string"
                elif "angle" in key.lower() or "size" in key.lower():
                    structure["type_requirements"][key] = "number"
                elif "list" in key.lower() or "array" in key.lower():
                    structure["type_requirements"][key] = "list"
            else:
                structure["optional_keys"].append(key)
                structure["type_requirements"][key] = type(value).__name__
        
        return structure
    
    def _generate_mapping_suggestions(self, source_structure: Dict[str, Any], 
                                    target_structure: Dict[str, Any],
                                    matching_rules: List[MappingRule]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ˜ å°„å»ºè®®"""
        suggestions = []
        
        # åŸºäºç»“æ„ç›¸ä¼¼æ€§ç”Ÿæˆå»ºè®®
        if source_structure["type"] == "dict" and target_structure["type"] == "dict":
            source_keys = source_structure.get("keys", [])
            target_keys = target_structure.get("required_keys", [])
            
            for target_key in target_keys:
                # å¯»æ‰¾æœ€ä½³åŒ¹é…çš„æºé”®
                best_match = self._find_best_key_match(target_key, source_keys)
                if best_match:
                    suggestions.append({
                        "type": "key_mapping",
                        "source_key": best_match,
                        "target_key": target_key,
                        "confidence": self._calculate_key_similarity(target_key, best_match)
                    })
        
        # åŸºäºç±»å‹è½¬æ¢ç”Ÿæˆå»ºè®®
        for rule in matching_rules:
            if rule.transformation:
                suggestions.append({
                    "type": "type_conversion",
                    "transformation": rule.transformation,
                    "priority": rule.priority
                })
        
        return suggestions
    
    def _find_best_key_match(self, target_key: str, source_keys: List[str]) -> Optional[str]:
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„æºé”®"""
        if not source_keys:
            return None
        
        # ç²¾ç¡®åŒ¹é…
        if target_key in source_keys:
            return target_key
        
        # æ¨¡ç³ŠåŒ¹é…
        best_match = None
        best_score = 0
        
        for source_key in source_keys:
            score = self._calculate_key_similarity(target_key, source_key)
            if score > best_score and score > 0.3:  # è®¾ç½®æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
                best_score = score
                best_match = source_key
        
        return best_match
    
    def _calculate_key_similarity(self, key1: str, key2: str) -> float:
        """è®¡ç®—é”®ç›¸ä¼¼åº¦"""
        if key1 == key2:
            return 1.0
        
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        k1, k2 = key1.lower(), key2.lower()
        
        # åŒ…å«å…³ç³»
        if k1 in k2 or k2 in k1:
            return 0.8
        
        # å…¬å…±å­—ç¬¦æ¯”ä¾‹
        common_chars = set(k1) & set(k2)
        total_chars = set(k1) | set(k2)
        if total_chars:
            return len(common_chars) / len(total_chars)
        
        return 0.0
    
    def _check_compatibility_issues(self, source_structure: Dict[str, Any], 
                                  target_structure: Dict[str, Any],
                                  suggested_mappings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ£€æŸ¥å…¼å®¹æ€§é—®é¢˜"""
        issues = {
            "missing_keys": [],
            "type_mismatches": []
        }
        
        if target_structure["type"] == "dict":
            target_keys = target_structure.get("required_keys", [])
            source_keys = source_structure.get("keys", [])
            
            for target_key in target_keys:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ å°„å»ºè®®
                has_mapping = any(
                    mapping["target_key"] == target_key 
                    for mapping in suggested_mappings 
                    if mapping["type"] == "key_mapping"
                )
                
                if not has_mapping:
                    issues["missing_keys"].append(target_key)
        
        return issues
    
    def _calculate_compatibility_confidence(self, analysis: Dict[str, Any]) -> float:
        """è®¡ç®—å…¼å®¹æ€§ç½®ä¿¡åº¦"""
        total_issues = len(analysis["missing_keys"]) + len(analysis["type_mismatches"])
        
        if total_issues == 0:
            return 1.0
        
        # åŸºäºé—®é¢˜æ•°é‡å’Œç±»å‹è®¡ç®—ç½®ä¿¡åº¦
        confidence = 1.0 - (total_issues * 0.2)
        return max(0.0, confidence)
    
    def _generate_adaptive_adapter_code(self, source_tool: str, target_tool: str, 
                                      source_output: 'NodeOutput', target_params: Dict[str, Any],
                                      analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆè‡ªé€‚åº”é€‚é…å™¨ä»£ç """
        adapter_name = f"{source_tool}_to_{target_tool}_adapter"
        
        # åˆ†ææºæ•°æ®å’Œç›®æ ‡æœŸæœ›
        source_structure = analysis.get("source_structure", {})
        target_structure = analysis.get("target_structure", {})
        
        # ç”Ÿæˆæ™ºèƒ½é€‚é…é€»è¾‘
        adaptation_logic = self._generate_adaptation_logic(source_structure, target_structure, target_params)
        
        code = f'''
import json
import logging
import tempfile
import os
from typing import Any, Dict, List

def {adapter_name}(source_data: Any) -> Any:
    """
    è‡ªé€‚åº”é€‚é…å™¨ï¼š{source_tool} -> {target_tool}
    åŸºäºç»“æ„åˆ†æè‡ªåŠ¨ç”Ÿæˆ
    """
    logger = logging.getLogger("{adapter_name}")
    
    try:
        # è¾“å…¥éªŒè¯
        if source_data is None:
            logger.warning("æºæ•°æ®ä¸ºç©º")
            return None
        
        # åˆ›å»ºè¾“å‡ºå‰¯æœ¬
        if isinstance(source_data, dict):
            output = source_data.copy()
        else:
            output = {{"data": source_data}}
        
        # åº”ç”¨æ˜ å°„è§„åˆ™
        mappings = {analysis["suggested_mappings"]}
        
        for mapping in mappings:
            if mapping["type"] == "key_mapping":
                source_key = mapping["source_key"]
                target_key = mapping["target_key"]
                
                if isinstance(output, dict) and source_key in output:
                    if target_key not in output:
                        output[target_key] = output[source_key]
                        logger.info(f"æ˜ å°„é”®: {{source_key}} -> {{target_key}}")
            
            elif mapping["type"] == "type_conversion":
                transformation = mapping["transformation"]
                logger.info(f"åº”ç”¨ç±»å‹è½¬æ¢: {{transformation}}")
        
        # æ™ºèƒ½ç»“æ„é€‚é…
        {adaptation_logic}
        
        logger.info("è‡ªé€‚åº”é€‚é…å™¨è½¬æ¢å®Œæˆ")
        return output
        
    except Exception as e:
        logger.error(f"è‡ªé€‚åº”é€‚é…å™¨è½¬æ¢å¤±è´¥: {{e}}")
        return source_data
'''
        
        return code
    
    def _generate_adaptation_logic(self, source_structure: Dict[str, Any], 
                                 target_structure: Dict[str, Any], 
                                 target_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ™ºèƒ½é€‚é…é€»è¾‘"""
        logic_parts = []
        
        # å¤„ç†PIL Imageå¯¹è±¡åˆ°æ–‡ä»¶è·¯å¾„çš„è½¬æ¢
        if source_structure.get("type") == "list" and source_structure.get("element_type") == "PIL_Image":
            logic_parts.append("""
        # å¤„ç†PIL Imageåˆ—è¡¨åˆ°æ–‡ä»¶è·¯å¾„çš„è½¬æ¢
        if isinstance(output, dict):
            for key, value in list(output.items()):
                if isinstance(value, list) and value:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºPIL Imageåˆ—è¡¨
                    if hasattr(value[0], 'save'):
                        # å°†PIL Imageåˆ—è¡¨è½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                        temp_paths = []
                        for i, img in enumerate(value):
                            if hasattr(img, 'save'):
                                temp_dir = tempfile.mkdtemp()
                                temp_path = os.path.join(temp_dir, f"image_{i}.png")
                                img.save(temp_path)
                                temp_paths.append(temp_path)
                        
                        # å¦‚æœåªæœ‰ä¸€ä¸ªå›¾ç‰‡ï¼Œè¿”å›å•ä¸ªè·¯å¾„ï¼›å¦åˆ™è¿”å›è·¯å¾„åˆ—è¡¨
                        if len(temp_paths) == 1:
                            output[key] = temp_paths[0]
                        else:
                            output[key] = temp_paths
                        
                        logger.info(f"å°†PIL Imageåˆ—è¡¨è½¬æ¢ä¸ºæ–‡ä»¶è·¯å¾„: {{key}}")
""")
        
        # å¤„ç†åˆ—è¡¨åˆ°å•ä¸ªå€¼çš„è½¬æ¢
        logic_parts.append("""
        # æ™ºèƒ½åˆ—è¡¨å¤„ç†
        if isinstance(output, dict):
            for key, value in list(output.items()):
                if isinstance(value, list):
                    if len(value) == 1:
                        # å•å…ƒç´ åˆ—è¡¨è§£åŒ…
                        output[key] = value[0]
                        logger.info(f"è§£åŒ…å•å…ƒç´ åˆ—è¡¨: {{key}}")
                    elif len(value) > 1:
                        # å¤šå…ƒç´ åˆ—è¡¨ä¿æŒåŸæ ·
                        pass
                elif not isinstance(value, list) and key.endswith("s"):
                    # å•å€¼åŒ…è£…ä¸ºåˆ—è¡¨
                    output[key] = [value]
                    logger.info(f"åŒ…è£…å•å€¼ä¸ºåˆ—è¡¨: {{key}}")
""")
        
        # å¤„ç†ç‰¹å®šå­—æ®µçš„æ™ºèƒ½æ˜ å°„
        if "image_path" in target_params or "images" in target_params:
            logic_parts.append("""
        # æ™ºèƒ½å›¾åƒè·¯å¾„æ˜ å°„
        if isinstance(output, dict):
            # å¯»æ‰¾å›¾åƒç›¸å…³çš„æ•°æ®
            image_data = None
            for key, value in output.items():
                if isinstance(value, list) and value:
                    if hasattr(value[0], 'save'):  # PIL Image
                        image_data = value
                        break
                    elif isinstance(value[0], str) and value[0].lower().endswith(('.png', '.jpg', '.jpeg')):
                        image_data = value
                        break
            
            # æ˜ å°„åˆ°æœŸæœ›çš„å­—æ®µ
            if image_data is not None:
                if "image_path" in output and not output["image_path"]:
                    output["image_path"] = image_data[0] if len(image_data) == 1 else image_data
                if "images" in output and not output["images"]:
                    output["images"] = image_data
                logger.info("æ™ºèƒ½æ˜ å°„å›¾åƒæ•°æ®")
""")
        
        return "\n".join(logic_parts)
    
    def _compile_adapter(self, adapter_name: str, code: str) -> Optional[Callable]:
        """ç¼–è¯‘é€‚é…å™¨ä»£ç """
        try:
            # åˆ›å»ºæœ¬åœ°å‘½åç©ºé—´
            local_namespace = {}
            
            # æ‰§è¡Œä»£ç 
            exec(code, globals(), local_namespace)
            
            # è·å–å‡½æ•°
            adapter_func = local_namespace.get(adapter_name)
            
            if adapter_func and callable(adapter_func):
                return adapter_func
            
        except Exception as e:
            self.logger.error(f"ç¼–è¯‘é€‚é…å™¨ {adapter_name} å¤±è´¥: {e}")
        
        return None
    
    def _intelligent_mapping(self, source_data: Any, source_structure: Dict[str, Any], 
                           target_expectation: Dict[str, Any]) -> Optional[Any]:
        """æ™ºèƒ½æ˜ å°„æ•°æ®"""
        try:
            self.logger.info(f"ğŸ” æ™ºèƒ½æ˜ å°„å¼€å§‹:")
            self.logger.info(f"   æºæ•°æ®ç±»å‹: {type(source_data)}")
            self.logger.info(f"   æºæ•°æ®ç»“æ„: {source_structure}")
            self.logger.info(f"   ç›®æ ‡æœŸæœ›: {target_expectation}")
            
            # å¤„ç†PIL Imageåˆ—è¡¨çš„ç‰¹æ®Šæƒ…å†µ
            if isinstance(source_data, list) and source_data:
                if hasattr(source_data[0], 'save'):  # PIL Imageåˆ—è¡¨
                    self.logger.info("   æ£€æµ‹åˆ°PIL Imageåˆ—è¡¨ï¼Œè½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶è·¯å¾„")
                    # å°†PIL Imageåˆ—è¡¨è½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
                    import tempfile
                    import os
                    
                    temp_paths = []
                    for i, img in enumerate(source_data):
                        if hasattr(img, 'save'):
                            temp_dir = tempfile.mkdtemp()
                            temp_path = os.path.join(temp_dir, f"image_{i}.png")
                            img.save(temp_path)
                            temp_paths.append(temp_path)
                    
                    # æ ¹æ®ç›®æ ‡æœŸæœ›è¿”å›åˆé€‚çš„æ ¼å¼
                    if "image_path" in target_expectation:
                        result = {"image_path": temp_paths[0] if len(temp_paths) == 1 else temp_paths}
                        self.logger.info(f"   è¿”å›image_pathæ ¼å¼: {result}")
                        return result
                    elif "images" in target_expectation:
                        result = {"images": temp_paths}
                        self.logger.info(f"   è¿”å›imagesæ ¼å¼: {result}")
                        return result
                    else:
                        result = {"data": temp_paths}
                        self.logger.info(f"   è¿”å›dataæ ¼å¼: {result}")
                        return result
            
            # å¤„ç†å­—å…¸æ•°æ®
            if isinstance(source_data, dict):
                self.logger.info("   æ£€æµ‹åˆ°å­—å…¸æ•°æ®ï¼Œè¿›è¡Œæ™ºèƒ½æ˜ å°„")
                mapped_data = source_data.copy()
                
                # åŸºäºç»“æ„åˆ†æè¿›è¡Œæ™ºèƒ½æ˜ å°„
                if source_structure["type"] == "dict":
                    source_keys = source_structure.get("keys", [])
                    self.logger.info(f"   æºå­—å…¸é”®: {source_keys}")
                    
                    for target_key in target_expectation:
                        self.logger.info(f"   å¤„ç†ç›®æ ‡é”®: {target_key}")
                        if target_key not in mapped_data:
                            # å¯»æ‰¾æœ€ä½³åŒ¹é…
                            best_match = self._find_best_key_match(target_key, source_keys)
                            self.logger.info(f"   æœ€ä½³åŒ¹é…é”®: {best_match}")
                            
                            # æ™ºèƒ½é€‰æ‹©æœ€ä½³åŒ¹é…
                            if best_match and best_match in mapped_data:
                                mapped_data[target_key] = mapped_data[best_match]
                                self.logger.info(f"   æ˜ å°„ {best_match} -> {target_key}: {mapped_data[target_key]}")
                            else:
                                self.logger.warning(f"   æœªæ‰¾åˆ°é”® {target_key} çš„åŒ¹é…")
                        else:
                            self.logger.info(f"   é”® {target_key} å·²å­˜åœ¨: {mapped_data[target_key]}")
                
                self.logger.info(f"   æœ€ç»ˆæ˜ å°„ç»“æœ: {mapped_data}")
                return mapped_data
            
            self.logger.warning(f"   ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(source_data)}")
            return None
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æ˜ å°„å¤±è´¥: {e}")
            return None

    def get_adapter_statistics(self) -> Dict[str, Any]:
        """è·å–é€‚é…å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.statistics.get_statistics()
        stats.update({
            "cache_hits": self._cache_hits,
            "cache_misses": self._cache_misses,
            "cache_hit_rate": self._cache_hits / (self._cache_hits + self._cache_misses) if (self._cache_hits + self._cache_misses) > 0 else 0,
            "total_adapters": len(self.adapters),
            "cached_adapters": len(self.adapter_cache)
        })
        return stats
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self.adapter_cache.clear()
        self.logger.info("ç¼“å­˜å·²æ¸…ç©º")
    
    def export_adapters(self, file_path: str):
        """å¯¼å‡ºé€‚é…å™¨å®šä¹‰"""
        try:
            export_data = {
                "adapters": {},
                "statistics": self.get_adapter_statistics(),
                "config": self.config.config
            }
            
            for name, adapter in self.adapters.items():
                export_data["adapters"][name] = {
                    "name": adapter.name,
                    "adapter_type": adapter.adapter_type.value,
                    "source_tool": adapter.source_tool,
                    "target_tool": adapter.target_tool,
                    "mapping_rules": [
                        {
                            "source_pattern": rule.source_pattern,
                            "target_pattern": rule.target_pattern,
                            "transformation": rule.transformation,
                            "priority": rule.priority
                        }
                        for rule in adapter.mapping_rules
                    ],
                    "code": adapter.code,
                    "is_active": adapter.is_active,
                    "created_at": adapter.created_at,
                    "metadata": adapter.metadata
                }
            
            import json
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"é€‚é…å™¨å®šä¹‰å·²å¯¼å‡ºåˆ°: {file_path}")
            
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºé€‚é…å™¨å¤±è´¥: {e}")
    
    def import_adapters(self, file_path: str):
        """å¯¼å…¥é€‚é…å™¨å®šä¹‰"""
        try:
            import json
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
            
            # å¯¼å…¥é…ç½®
            if "config" in import_data:
                self.config.update_config(**import_data["config"])
            
            # å¯¼å…¥é€‚é…å™¨
            for name, adapter_data in import_data.get("adapters", {}).items():
                adapter_def = AdapterDefinition(
                    name=adapter_data["name"],
                    adapter_type=AdapterType(adapter_data["adapter_type"]),
                    source_tool=adapter_data["source_tool"],
                    target_tool=adapter_data["target_tool"],
                    mapping_rules=[
                        MappingRule(
                            source_pattern=rule["source_pattern"],
                            target_pattern=rule["target_pattern"],
                            transformation=rule["transformation"],
                            priority=rule["priority"]
                        )
                        for rule in adapter_data["mapping_rules"]
                    ],
                    code=adapter_data["code"],
                    is_active=adapter_data["is_active"],
                    created_at=adapter_data["created_at"],
                    metadata=adapter_data.get("metadata", {})
                )
                
                self.adapters[name] = adapter_def
                
                # ç¼–è¯‘é€‚é…å™¨
                if adapter_def.code:
                    adapter_func = self._compile_adapter(name, adapter_def.code)
                    if adapter_func:
                        self.adapter_cache[name] = adapter_func
            
            self.logger.info(f"æˆåŠŸå¯¼å…¥ {len(import_data.get('adapters', {}))} ä¸ªé€‚é…å™¨")
            
        except Exception as e:
            self.logger.error(f"å¯¼å…¥é€‚é…å™¨å¤±è´¥: {e}")
    
    def get_adapter_info(self, adapter_name: str) -> Optional[Dict[str, Any]]:
        """è·å–é€‚é…å™¨è¯¦ç»†ä¿¡æ¯"""
        if adapter_name not in self.adapters:
            return None
        
        adapter = self.adapters[adapter_name]
        usage_stats = self.statistics.adapter_usage.get(adapter_name, {})
        
        return {
            "name": adapter.name,
            "adapter_type": adapter.adapter_type.value,
            "source_tool": adapter.source_tool,
            "target_tool": adapter.target_tool,
            "is_active": adapter.is_active,
            "created_at": adapter.created_at,
            "usage_count": usage_stats.get("success", 0) + usage_stats.get("failed", 0),
            "success_rate": usage_stats.get("success", 0) / (usage_stats.get("success", 0) + usage_stats.get("failed", 0)) if (usage_stats.get("success", 0) + usage_stats.get("failed", 0)) > 0 else 0,
            "total_time": usage_stats.get("total_time", 0),
            "mapping_rules": [
                {
                    "source_pattern": rule.source_pattern,
                    "target_pattern": rule.target_pattern,
                    "transformation": rule.transformation,
                    "priority": rule.priority
                }
                for rule in adapter.mapping_rules
            ],
            "metadata": adapter.metadata
        }
    
    def list_adapters(self, active_only: bool = False) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰é€‚é…å™¨"""
        adapters = []
        for name in self.adapters:
            adapter_info = self.get_adapter_info(name)
            if adapter_info and (not active_only or adapter_info["is_active"]):
                adapters.append(adapter_info)
        return adapters
    
    def enable_adapter(self, adapter_name: str):
        """å¯ç”¨é€‚é…å™¨"""
        if adapter_name in self.adapters:
            self.adapters[adapter_name].is_active = True
            self.logger.info(f"é€‚é…å™¨ {adapter_name} å·²å¯ç”¨")
        else:
            self.logger.warning(f"é€‚é…å™¨ {adapter_name} ä¸å­˜åœ¨")
    
    def disable_adapter(self, adapter_name: str):
        """ç¦ç”¨é€‚é…å™¨"""
        if adapter_name in self.adapters:
            self.adapters[adapter_name].is_active = False
            self.logger.info(f"é€‚é…å™¨ {adapter_name} å·²ç¦ç”¨")
        else:
            self.logger.warning(f"é€‚é…å™¨ {adapter_name} ä¸å­˜åœ¨")
    
    def delete_adapter(self, adapter_name: str):
        """åˆ é™¤é€‚é…å™¨"""
        if adapter_name in self.adapters:
            del self.adapters[adapter_name]
            if adapter_name in self.adapter_cache:
                del self.adapter_cache[adapter_name]
            self.logger.info(f"é€‚é…å™¨ {adapter_name} å·²åˆ é™¤")
        else:
            self.logger.warning(f"é€‚é…å™¨ {adapter_name} ä¸å­˜åœ¨")
    
    def validate_adapter(self, adapter_name: str) -> Dict[str, Any]:
        """éªŒè¯é€‚é…å™¨"""
        if adapter_name not in self.adapters:
            return {"valid": False, "error": "é€‚é…å™¨ä¸å­˜åœ¨"}
        
        adapter = self.adapters[adapter_name]
        validation_result = {"valid": True, "warnings": [], "errors": []}
        
        # æ£€æŸ¥ä»£ç è¯­æ³•
        try:
            compile(adapter.code, '<string>', 'exec')
        except SyntaxError as e:
            validation_result["valid"] = False
            validation_result["errors"].append(f"è¯­æ³•é”™è¯¯: {e}")
        
        # æ£€æŸ¥æ˜ å°„è§„åˆ™
        for rule in adapter.mapping_rules:
            if not rule.source_pattern or not rule.target_pattern:
                validation_result["warnings"].append("æ˜ å°„è§„åˆ™æ¨¡å¼ä¸ºç©º")
        
        # æ£€æŸ¥è½¬æ¢å™¨
        for rule in adapter.mapping_rules:
            if rule.transformation and rule.transformation not in self.transformation_engine.transformers:
                validation_result["warnings"].append(f"æœªçŸ¥çš„è½¬æ¢å™¨: {rule.transformation}")
        
        return validation_result
    
    def benchmark_adapter(self, adapter_name: str, test_data: Any, iterations: int = 100) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if adapter_name not in self.adapter_cache:
            return {"error": "é€‚é…å™¨ä¸å­˜åœ¨æˆ–æœªç¼–è¯‘"}
        
        adapter_func = self.adapter_cache[adapter_name]
        times = []
        
        for _ in range(iterations):
            start_time = time.time()
            try:
                result = adapter_func(test_data)
                end_time = time.time()
                times.append(end_time - start_time)
            except Exception as e:
                return {"error": f"æ‰§è¡Œå¤±è´¥: {e}"}
        
        return {
            "iterations": iterations,
            "total_time": sum(times),
            "average_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "std_deviation": (sum((t - sum(times)/len(times))**2 for t in times) / len(times))**0.5
        }

# å…¨å±€é€‚é…å™¨å®ä¾‹
_global_adapter = None

def get_tool_adapter() -> ToolAdapter:
    """è·å–å…¨å±€å·¥å…·é€‚é…å™¨å®ä¾‹"""
    global _global_adapter
    if _global_adapter is None:
        _global_adapter = ToolAdapter()
    return _global_adapter