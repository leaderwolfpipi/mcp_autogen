#!/usr/bin/env python3
"""
å ä½ç¬¦è§£æå™¨
ç”¨äºè§£æå’Œæ›¿æ¢pipelineä¸­çš„å ä½ç¬¦å¼•ç”¨
"""

import re
import logging
import os
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
from collections import defaultdict

# å¯¼å…¥å·¥å…·é€‚é…å™¨
from .tool_adapter import get_tool_adapter
from .semantic_dependency_analyzer import SemanticDependencyAnalyzer
from .smart_parameter_adapter import SmartParameterAdapter

@dataclass
class NodeOutput:
    """èŠ‚ç‚¹è¾“å‡ºä¿¡æ¯"""
    node_id: str
    output_type: str
    output_key: str
    value: Any
    description: str

class PlaceholderResolver:
    """å ä½ç¬¦è§£æå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger("PlaceholderResolver")
        # å ä½ç¬¦æ¨¡å¼ï¼š$node_id.output æˆ– $node_id.output.key æˆ– $node_id.output.key1.key2...
        self.placeholder_pattern = r'\$([a-zA-Z_][a-zA-Z0-9_]*)\.output(?:\.([a-zA-Z_][a-zA-Z0-9_\.]*))?'
        
        # åˆå§‹åŒ–å·¥å…·é€‚é…å™¨
        self.tool_adapter = get_tool_adapter()
        
        # åˆå§‹åŒ–è¯­ä¹‰ä¾èµ–åˆ†æå™¨
        self.semantic_analyzer = SemanticDependencyAnalyzer()
        
        # åˆå§‹åŒ–æ™ºèƒ½å‚æ•°é€‚é…å™¨
        self.parameter_adapter = SmartParameterAdapter()
        
    def resolve_placeholders(self, params: Dict[str, Any], node_outputs: Dict[str, NodeOutput]) -> Dict[str, Any]:
        """
        è§£æå‚æ•°ä¸­çš„å ä½ç¬¦å¹¶æ›¿æ¢ä¸ºå®é™…å€¼
        åŒ…å«æ™ºèƒ½å‚æ•°é€‚é…åŠŸèƒ½
        
        Args:
            params: åŒ…å«å ä½ç¬¦çš„å‚æ•°å­—å…¸
            node_outputs: èŠ‚ç‚¹è¾“å‡ºæ˜ å°„ {node_id: NodeOutput}
            
        Returns:
            è§£æåçš„å‚æ•°å­—å…¸
        """
        # é¦–å…ˆè¿›è¡Œå ä½ç¬¦è§£æ
        resolved_params = {}
        
        for key, value in params.items():
            if isinstance(value, str):
                resolved_value = self._resolve_string_placeholder(value, node_outputs)
                resolved_params[key] = resolved_value
            elif isinstance(value, dict):
                resolved_params[key] = self.resolve_placeholders(value, node_outputs)
            elif isinstance(value, list):
                resolved_params[key] = [
                    self.resolve_placeholders(item, node_outputs) if isinstance(item, dict)
                    else self._resolve_string_placeholder(item, node_outputs) if isinstance(item, str)
                    else item
                    for item in value
                ]
            else:
                resolved_params[key] = value
        
        # ç„¶åè¿›è¡Œæ™ºèƒ½å‚æ•°é€‚é…
        adapted_params = self._adapt_parameters_intelligently(resolved_params, node_outputs)
        
        return adapted_params
    
    def _adapt_parameters_intelligently(self, params: Dict[str, Any], 
                                      node_outputs: Dict[str, NodeOutput]) -> Dict[str, Any]:
        """æ™ºèƒ½å‚æ•°é€‚é…"""
        # åˆ†æå‚æ•°è¯­ä¹‰ä¸åŒ¹é…
        mismatches = []
        for param_name, param_value in params.items():
            param_semantic = self._analyze_param_semantic(param_name, param_value)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‚é…
            if self._needs_semantic_adaptation(param_name, param_value, param_semantic):
                adaptation = self._create_semantic_adaptation(param_name, param_value, param_semantic)
                if adaptation:
                    params[param_name] = adaptation
                    self.logger.info(f"æ™ºèƒ½å‚æ•°é€‚é…: {param_name} -> {type(adaptation).__name__}")
        
        return params
    
    def _analyze_param_semantic(self, param_name: str, param_value: Any) -> str:
        """åˆ†æå‚æ•°è¯­ä¹‰"""
        # åŸºäºå‚æ•°åç§°åˆ†æ
        if any(keyword in param_name.lower() for keyword in ['file', 'path', 'filename']):
            return "file_path"
        elif any(keyword in param_name.lower() for keyword in ['content', 'text', 'data']):
            return "file_content"
        elif any(keyword in param_name.lower() for keyword in ['url', 'link']):
            return "url"
        
        # åŸºäºå‚æ•°å€¼åˆ†æ
        if isinstance(param_value, str):
            if param_value.startswith(('http://', 'https://')):
                return "url"
            elif os.path.sep in param_value or param_value.endswith(('.txt', '.md', '.pdf')):
                return "file_path"
            elif len(param_value) > 100:  # é•¿æ–‡æœ¬å¯èƒ½æ˜¯å†…å®¹
                return "file_content"
        elif isinstance(param_value, dict):
            # åˆ†æå­—å…¸ç»“æ„
            return self._analyze_dict_semantic(param_value, param_name)
        
        return "unknown"
    
    def _analyze_dict_semantic(self, data: Dict[str, Any], param_name: str) -> str:
        """åˆ†æå­—å…¸çš„è¯­ä¹‰ç±»å‹"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        if "file_path" in data:
            return "file_path"
        elif "content" in data or "text" in data:
            return "file_content"
        elif "url" in data or "link" in data:
            return "url"
        elif "status" in data and "message" in data:
            return "metadata"
        
        # æ£€æŸ¥å­—å…¸çš„å€¼ç±»å‹
        for key, value in data.items():
            if isinstance(value, str):
                if value.endswith(('.txt', '.md', '.pdf')) or os.path.sep in value:
                    return "file_path"
                elif len(value) > 100:
                    return "file_content"
        
        return "unknown"
    
    def _needs_semantic_adaptation(self, param_name: str, param_value: Any, param_semantic: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¯­ä¹‰é€‚é…"""
        # å¦‚æœå‚æ•°åæš—ç¤ºéœ€è¦æ–‡ä»¶è·¯å¾„ï¼Œä½†å€¼æ˜¯å†…å®¹
        if param_semantic == "file_content" and any(keyword in param_name.lower() for keyword in ['file', 'path']):
            return True
        
        # å¦‚æœå‚æ•°åæš—ç¤ºéœ€è¦å†…å®¹ï¼Œä½†å€¼æ˜¯æ–‡ä»¶è·¯å¾„
        if param_semantic == "file_path" and any(keyword in param_name.lower() for keyword in ['content', 'text']):
            return True
        
        # å¦‚æœå‚æ•°åæš—ç¤ºéœ€è¦æ–‡ä»¶è·¯å¾„ï¼Œä½†å€¼æ˜¯å­—å…¸
        if param_semantic == "file_path" and isinstance(param_value, dict):
            return True
        
        return False
    
    def _create_semantic_adaptation(self, param_name: str, param_value: Any, param_semantic: str) -> Any:
        """åˆ›å»ºè¯­ä¹‰é€‚é…"""
        # æ–‡ä»¶å†…å®¹ -> æ–‡ä»¶è·¯å¾„çš„é€‚é…
        if param_semantic == "file_content" and any(keyword in param_name.lower() for keyword in ['file', 'path']):
            return self._content_to_file_path(param_value, param_name)
        
        # æ–‡ä»¶è·¯å¾„ -> æ–‡ä»¶å†…å®¹çš„é€‚é…
        elif param_semantic == "file_path" and any(keyword in param_name.lower() for keyword in ['content', 'text']):
            return self._file_path_to_content(param_value)
        
        # å­—å…¸ -> æ–‡ä»¶è·¯å¾„çš„é€‚é…ï¼ˆæ–°å¢ï¼‰
        elif param_semantic == "file_path" and isinstance(param_value, dict):
            extracted_path = self._extract_file_path_from_dict(param_value)
            if extracted_path:
                return extracted_path
        
        return param_value
    
    def _content_to_file_path(self, content: str, param_name: str) -> str:
        """å°†æ–‡ä»¶å†…å®¹è½¬æ¢ä¸ºæ–‡ä»¶è·¯å¾„"""
        try:
            # ç”Ÿæˆåˆé€‚çš„æ–‡ä»¶å
            if content.startswith('# '):
                # ä»Markdownæ ‡é¢˜æå–æ–‡ä»¶å
                title_match = re.match(r'^#\s+(.+)$', content.split('\n')[0])
                if title_match:
                    filename = re.sub(r'[^\w\s-]', '', title_match.group(1)).strip()
                    filename = re.sub(r'[-\s]+', '_', filename)
                    return f"{filename}.md"
            
            # é»˜è®¤æ–‡ä»¶å
            return "generated_content.md"
        
        except Exception as e:
            self.logger.warning(f"å†…å®¹è½¬æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return "generated_content.md"
    
    def _file_path_to_content(self, file_path: str) -> str:
        """å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºæ–‡ä»¶å†…å®¹"""
        try:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›è·¯å¾„æœ¬èº«ä½œä¸ºå†…å®¹
                return f"æ–‡ä»¶è·¯å¾„: {file_path}"
        
        except Exception as e:
            self.logger.warning(f"æ–‡ä»¶è·¯å¾„è½¬å†…å®¹å¤±è´¥: {e}")
            return f"æ–‡ä»¶è·¯å¾„: {file_path}"
    
    def _extract_file_path_from_dict(self, data: Dict[str, Any]) -> Optional[str]:
        """ä»å­—å…¸ä¸­æå–æ–‡ä»¶è·¯å¾„"""
        try:
            # ç›´æ¥æŸ¥æ‰¾ file_path é”®
            if "file_path" in data:
                file_path = data["file_path"]
                if isinstance(file_path, str):
                    return file_path
                elif isinstance(file_path, dict) and "file_path" in file_path:
                    return file_path["file_path"]
            
            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ–‡ä»¶è·¯å¾„é”®
            for key in ["path", "filename", "file", "location"]:
                if key in data:
                    value = data[key]
                    if isinstance(value, str) and (value.endswith(('.txt', '.md', '.pdf')) or os.path.sep in value):
                        return value
            
            # æŸ¥æ‰¾åŒ…å«æ–‡ä»¶è·¯å¾„çš„åµŒå¥—å­—å…¸
            for key, value in data.items():
                if isinstance(value, dict):
                    nested_path = self._extract_file_path_from_dict(value)
                    if nested_path:
                        return nested_path
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ä»å­—å…¸æå–æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return None
    
    def _resolve_string_placeholder(self, value: str, node_outputs: Dict[str, NodeOutput]) -> Any:
        """
        è§£æå­—ç¬¦ä¸²ä¸­çš„å ä½ç¬¦
        
        Args:
            value: å¯èƒ½åŒ…å«å ä½ç¬¦çš„å­—ç¬¦ä¸²
            node_outputs: èŠ‚ç‚¹è¾“å‡ºæ˜ å°„
            
        Returns:
            è§£æåçš„å€¼
        """
        if not isinstance(value, str):
            return value
            
        # æŸ¥æ‰¾æ‰€æœ‰å ä½ç¬¦
        matches = re.finditer(self.placeholder_pattern, value)
        
        if not matches:
            return value
            
        resolved_value = value
        
        for match in matches:
            node_id = match.group(1)
            output_key = match.group(2)  # å¯èƒ½ä¸ºNone
            
            if node_id not in node_outputs:
                self.logger.warning(f"èŠ‚ç‚¹ {node_id} çš„è¾“å‡ºæœªæ‰¾åˆ°")
                continue
                
            node_output = node_outputs[node_id]
            
            # ç¡®å®šè¦ä½¿ç”¨çš„å€¼
            if output_key:
                # å¦‚æœæŒ‡å®šäº†å…·ä½“çš„è¾“å‡ºé”®ï¼Œå°è¯•ä»è¾“å‡ºå€¼ä¸­è·å–
                self.logger.info(f"å°è¯•æå–åµŒå¥—å€¼: {output_key} ä»èŠ‚ç‚¹ {node_id}")
                replacement_value = self._extract_nested_value(node_output.value, output_key)
                self.logger.info(f"æå–ç»“æœ: {replacement_value}")
                
                # å¦‚æœç›´æ¥æå–å¤±è´¥ï¼Œå°è¯•åœ¨dataå­å­—å…¸ä¸­æŸ¥æ‰¾
                if replacement_value is None and isinstance(node_output.value, dict) and 'data' in node_output.value:
                    self.logger.info(f"å°è¯•åœ¨dataå­å­—å…¸ä¸­æŸ¥æ‰¾é”®: {output_key}")
                    replacement_value = self._extract_nested_value(node_output.value['data'], output_key)
                    self.logger.info(f"åœ¨dataä¸­æŸ¥æ‰¾ç»“æœ: {replacement_value}")
                
                # å‘åå…¼å®¹ï¼šå¤„ç†æ—§æ ¼å¼å­—æ®µæ˜ å°„
                if replacement_value is None and isinstance(node_output.value, dict):
                    # æœç´¢ç±»å·¥å…·çš„å­—æ®µæ˜ å°„
                    search_field_mappings = {
                        "results": ["data.primary", "results"],
                        "primary": ["data.primary", "results"],
                        "rotated_images": ["data.primary", "rotated_images"],
                        "scaled_images": ["data.primary", "scaled_images"],
                        "paths": ["paths", "data.primary"]
                    }
                    
                    if output_key in search_field_mappings:
                        for field_path in search_field_mappings[output_key]:
                            self.logger.info(f"å°è¯•å‘åå…¼å®¹å­—æ®µæ˜ å°„: {field_path}")
                            replacement_value = self._extract_nested_value(node_output.value, field_path)
                            if replacement_value is not None:
                                self.logger.info(f"å‘åå…¼å®¹æ˜ å°„æˆåŠŸ: {field_path} -> {replacement_value}")
                                break
                
                if replacement_value is None:
                    # å°è¯•ä½¿ç”¨å·¥å…·é€‚é…å™¨è‡ªåŠ¨é€‚é…
                    self.logger.info(f"å°è¯•è‡ªåŠ¨é€‚é…èŠ‚ç‚¹ {node_id} çš„è¾“å‡ºä»¥åŒ¹é…é”® {output_key}")
                    adapted_output = self.tool_adapter.auto_adapt_output(
                        node_output, 
                        {output_key: "expected_type"}  # ç®€åŒ–çš„æœŸæœ›ç»“æ„
                    )
                    
                    if isinstance(adapted_output, dict) and output_key in adapted_output:
                        replacement_value = adapted_output[output_key]
                        self.logger.info(f"è‡ªåŠ¨é€‚é…æˆåŠŸï¼Œæ‰¾åˆ°é”® {output_key}")
                    else:
                        self.logger.warning(f"èŠ‚ç‚¹ {node_id} çš„è¾“å‡ºä¸­æ²¡æœ‰é”® {output_key}ï¼Œä¸”è‡ªåŠ¨é€‚é…å¤±è´¥")
                        continue
            else:
                # æ²¡æœ‰æŒ‡å®šå…·ä½“é”®ï¼Œä½¿ç”¨æ™ºèƒ½æå–
                replacement_value = self._extract_primary_value(node_output)
                
            # æ›¿æ¢å ä½ç¬¦
            placeholder = match.group(0)
            # å¦‚æœå ä½ç¬¦æ˜¯æ•´ä¸ªå€¼ï¼Œç›´æ¥è¿”å›æ›¿æ¢å€¼
            if resolved_value.strip() == placeholder:
                return replacement_value
            else:
                # å¦åˆ™è¿›è¡Œå­—ç¬¦ä¸²æ›¿æ¢
                resolved_value = resolved_value.replace(placeholder, str(replacement_value))
            
        return resolved_value
    
    def _extract_nested_value(self, data: Any, key_path: str) -> Any:
        """
        ä»åµŒå¥—æ•°æ®ç»“æ„ä¸­æå–å€¼
        
        Args:
            data: æ•°æ®å¯¹è±¡
            key_path: é”®è·¯å¾„ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„åµŒå¥—è·¯å¾„
            
        Returns:
            æå–çš„å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        self.logger.info(f"_extract_nested_value: æ•°æ®={data}, é”®è·¯å¾„={key_path}")
        
        if not isinstance(data, dict):
            self.logger.info(f"_extract_nested_value: æ•°æ®ä¸æ˜¯å­—å…¸ï¼Œè¿”å›None")
            return None
            
        # åˆ†å‰²é”®è·¯å¾„
        keys = key_path.split('.')
        current = data
        
        self.logger.info(f"_extract_nested_value: é”®è·¯å¾„åˆ†å‰²={keys}")
        
        for key in keys:
            self.logger.info(f"_extract_nested_value: å¤„ç†é”®={key}, å½“å‰å€¼={current}")
            if isinstance(current, dict) and key in current:
                current = current[key]
                self.logger.info(f"_extract_nested_value: æ‰¾åˆ°é”®{key}, æ–°å€¼={current}")
            else:
                self.logger.info(f"_extract_nested_value: é”®{key}ä¸å­˜åœ¨æˆ–å½“å‰å€¼ä¸æ˜¯å­—å…¸ï¼Œè¿”å›None")
                return None
                
        self.logger.info(f"_extract_nested_value: æœ€ç»ˆç»“æœ={current}")
        return current
    
    def _extract_primary_value(self, node_output: NodeOutput) -> Any:
        """
        ä»èŠ‚ç‚¹è¾“å‡ºä¸­æå–ä¸»è¦å€¼ - çœŸæ­£é€šç”¨çš„å®ç°
        
        Args:
            node_output: èŠ‚ç‚¹è¾“å‡ºå¯¹è±¡
            
        Returns:
            æå–çš„ä¸»è¦å€¼
        """
        # å¦‚æœè¾“å‡ºæ˜¯å­—å…¸ï¼Œå°è¯•ä½¿ç”¨output_key
        if isinstance(node_output.value, dict):
            if node_output.output_key in node_output.value:
                return node_output.value[node_output.output_key]
            # å¦‚æœoutput_keyä¸å­˜åœ¨ï¼Œè¿”å›æ•´ä¸ªå­—å…¸
            return node_output.value
        
        # å¦‚æœè¾“å‡ºä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è¿”å›
        return node_output.value
    
    def validate_pipeline_dependencies(self, components: List[Dict[str, Any]]) -> List[str]:
        """
        éªŒè¯pipelineä¸­çš„ä¾èµ–å…³ç³»
        
        Args:
            components: pipelineç»„ä»¶åˆ—è¡¨
            
        Returns:
            é”™è¯¯ä¿¡æ¯åˆ—è¡¨
        """
        errors = []
        node_ids = {comp["id"] for comp in components}
        
        for i, component in enumerate(components):
            params = component.get("params", {})
            
            # æ£€æŸ¥å‚æ•°ä¸­çš„å ä½ç¬¦å¼•ç”¨
            placeholder_refs = self._extract_placeholder_references(params)
            
            for ref in placeholder_refs:
                node_id = ref["node_id"]
                
                # æ£€æŸ¥å¼•ç”¨çš„èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
                if node_id not in node_ids:
                    errors.append(f"ç»„ä»¶ {component['id']} å¼•ç”¨äº†ä¸å­˜åœ¨çš„èŠ‚ç‚¹: {node_id}")
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†åç»­èŠ‚ç‚¹ï¼ˆå¾ªç¯ä¾èµ–ï¼‰
                ref_index = next((j for j, c in enumerate(components) if c["id"] == node_id), -1)
                if ref_index > i:
                    errors.append(f"ç»„ä»¶ {component['id']} å¼•ç”¨äº†åç»­èŠ‚ç‚¹ {node_id}ï¼Œå¯èƒ½é€ æˆå¾ªç¯ä¾èµ–")
                    
        return errors
    
    def _extract_placeholder_references(self, params: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        æå–å‚æ•°ä¸­çš„æ‰€æœ‰å ä½ç¬¦å¼•ç”¨
        
        Args:
            params: å‚æ•°å­—å…¸
            
        Returns:
            å ä½ç¬¦å¼•ç”¨åˆ—è¡¨
        """
        references = []
        
        def extract_from_value(value):
            if isinstance(value, str):
                matches = re.finditer(self.placeholder_pattern, value)
                for match in matches:
                    references.append({
                        "node_id": match.group(1),
                        "output_key": match.group(2)
                    })
            elif isinstance(value, dict):
                for v in value.values():
                    extract_from_value(v)
            elif isinstance(value, list):
                for v in value:
                    extract_from_value(v)
        
        extract_from_value(params)
        return references
    
    def build_execution_order(self, components: List[Dict[str, Any]]) -> List[str]:
        """
        æ„å»ºpipelineçš„æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰
        ä½¿ç”¨è¯­ä¹‰ä¾èµ–åˆ†æç¡®ä¿é€šç”¨æ€§
        
        Args:
            components: pipelineç»„ä»¶åˆ—è¡¨
            
        Returns:
            æŒ‰ä¾èµ–é¡ºåºæ’åˆ—çš„èŠ‚ç‚¹IDåˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹æ„å»ºæ‰§è¡Œé¡ºåºï¼ŒèŠ‚ç‚¹æ•°é‡: {len(components)}")
        
        # é¦–å…ˆå°è¯•ä¼ ç»Ÿçš„å ä½ç¬¦å¼•ç”¨åˆ†æ
        traditional_order = self._build_traditional_execution_order(components)
        
        # ç„¶åä½¿ç”¨è¯­ä¹‰ä¾èµ–åˆ†æ
        semantic_order = self.semantic_analyzer.build_execution_order(components)
        
        # æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ç»“æœ
        self.logger.info(f"ğŸ“Š ä¼ ç»Ÿæ–¹æ³•æ‰§è¡Œé¡ºåº: {' -> '.join(traditional_order)}")
        self.logger.info(f"ğŸ“Š è¯­ä¹‰åˆ†ææ–¹æ³•æ‰§è¡Œé¡ºåº: {' -> '.join(semantic_order)}")
        
        # é€‰æ‹©æ›´åˆç†çš„æ‰§è¡Œé¡ºåº
        final_order = self._select_best_execution_order(components, traditional_order, semantic_order)
        
        self.logger.info(f"ğŸ“‹ æœ€ç»ˆæ‰§è¡Œé¡ºåº: {' -> '.join(final_order)}")
        return final_order
    
    def _build_traditional_execution_order(self, components: List[Dict[str, Any]]) -> List[str]:
        """ä¼ ç»Ÿçš„åŸºäºå ä½ç¬¦å¼•ç”¨çš„æ‰§è¡Œé¡ºåºæ„å»º"""
        # æ„å»ºä¾èµ–å›¾
        dependencies = {}
        node_ids = {comp["id"] for comp in components}
        
        for component in components:
            node_id = component["id"]
            dependencies[node_id] = set()
            
            # æ£€æŸ¥è¯¥èŠ‚ç‚¹ä¾èµ–çš„å…¶ä»–èŠ‚ç‚¹
            params = component.get("params", {})
            placeholder_refs = self._extract_placeholder_references(params)
            
            for ref in placeholder_refs:
                if ref["node_id"] in node_ids:
                    dependencies[node_id].add(ref["node_id"])
        
        # æ‹“æ‰‘æ’åº - ä¿®å¤å¾ªç¯ä¾èµ–æ£€æµ‹
        execution_order = []
        visited = set()
        temp_visited = set()
        
        def visit(node_id):
            """æ·±åº¦ä¼˜å…ˆæœç´¢ï¼Œæ£€æµ‹å¾ªç¯ä¾èµ–"""
            if node_id in temp_visited:
                # æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œè®°å½•ä½†ä¸æŠ›å‡ºå¼‚å¸¸
                self.logger.warning(f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {node_id}")
                return False
            if node_id in visited:
                return True
                
            temp_visited.add(node_id)
            
            # è®¿é—®æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹
            for dep in dependencies.get(node_id, []):
                if not visit(dep):
                    temp_visited.remove(node_id)
                    return False
                
            temp_visited.remove(node_id)
            visited.add(node_id)
            execution_order.append(node_id)
            return True
        
        # å°è¯•æ‹“æ‰‘æ’åº
        success = True
        for node_id in node_ids:
            if node_id not in visited:
                if not visit(node_id):
                    success = False
                    break
        
        # å¦‚æœæ‹“æ‰‘æ’åºå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ’åº
        if not success or len(execution_order) != len(node_ids):
            self.logger.warning("æ‹“æ‰‘æ’åºå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ’åº")
            execution_order = self._heuristic_execution_order(components, dependencies)
        
        return execution_order
    
    def _heuristic_execution_order(self, components: List[Dict[str, Any]], 
                                 dependencies: Dict[str, Set[str]]) -> List[str]:
        """å¯å‘å¼æ‰§è¡Œé¡ºåºæ„å»º"""
        node_ids = {comp["id"] for comp in components}
        
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å…¥åº¦å’Œå‡ºåº¦
        in_degree = defaultdict(int)
        out_degree = defaultdict(int)
        
        for node_id in node_ids:
            in_degree[node_id] = len(dependencies.get(node_id, set()))
            for dep in dependencies.get(node_id, set()):
                out_degree[dep] += 1
        
        # åŸºäºå·¥å…·ç±»å‹çš„ä¼˜å…ˆçº§
        tool_priority = {
            "data_source": 1,      # æ•°æ®æºä¼˜å…ˆ
            "data_processor": 2,   # æ•°æ®å¤„ç†å™¨
            "file_operator": 3,    # æ–‡ä»¶æ“ä½œ
            "storage": 4           # å­˜å‚¨æœ€å
        }
        
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ç»¼åˆä¼˜å…ˆçº§
        node_priorities = {}
        for comp in components:
            node_id = comp["id"]
            tool_type = comp.get("tool_type", "")
            
            # åŸºç¡€ä¼˜å…ˆçº§ï¼ˆåŸºäºå·¥å…·ç±»å‹ï¼‰
            base_priority = self._get_tool_category_priority(tool_type, tool_priority)
            
            # ä¾èµ–ä¼˜å…ˆçº§ï¼ˆå…¥åº¦è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
            dep_priority = -in_degree[node_id]
            
            # è¾“å‡ºä¼˜å…ˆçº§ï¼ˆå‡ºåº¦è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
            output_priority = out_degree[node_id]
            
            # ç»¼åˆä¼˜å…ˆçº§
            node_priorities[node_id] = (base_priority, dep_priority, output_priority, node_id)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_nodes = sorted(node_priorities.values(), key=lambda x: (x[0], x[1], x[2], x[3]))
        execution_order = [node[3] for node in sorted_nodes]
        
        return execution_order
    
    def _get_tool_category_priority(self, tool_type: str, tool_priority: Dict[str, int]) -> int:
        """è·å–å·¥å…·ç±»å‹çš„åŸºç¡€ä¼˜å…ˆçº§"""
        # å·¥å…·ç±»å‹åˆ†ç±»
        tool_categories = {
            "data_source": {"search_tool", "smart_search", "web_searcher"},
            "data_processor": {"enhanced_report_generator", "report_generator", "text_processor"},
            "file_operator": {"file_writer", "file_uploader", "minio_uploader"},
            "storage": {"minio_uploader", "file_uploader"}
        }
        
        # æŸ¥æ‰¾å·¥å…·ç±»å‹æ‰€å±çš„ç±»åˆ«
        for category, tools in tool_categories.items():
            if tool_type in tools:
                return tool_priority.get(category, 5)  # é»˜è®¤ä¼˜å…ˆçº§5
        
        return 5  # æœªçŸ¥å·¥å…·ç±»å‹é»˜è®¤ä¼˜å…ˆçº§
    
    def _select_best_execution_order(self, components: List[Dict[str, Any]], 
                                   traditional_order: List[str], 
                                   semantic_order: List[str]) -> List[str]:
        """é€‰æ‹©æœ€ä½³çš„æ‰§è¡Œé¡ºåº"""
        # å¦‚æœä¸¤ç§æ–¹æ³•ç»“æœç›¸åŒï¼Œç›´æ¥è¿”å›
        if traditional_order == semantic_order:
            self.logger.info("âœ… ä¸¤ç§æ–¹æ³•ç»“æœä¸€è‡´ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            return traditional_order
        
        # è®¡ç®—ä¼ ç»Ÿæ–¹æ³•çš„ä¾èµ–è¦†ç›–ç‡
        traditional_coverage = self._calculate_dependency_coverage(components, traditional_order)
        
        # è®¡ç®—è¯­ä¹‰åˆ†ææ–¹æ³•çš„ä¾èµ–è¦†ç›–ç‡
        semantic_coverage = self._calculate_dependency_coverage(components, semantic_order)
        
        self.logger.info(f"ğŸ“Š ä¼ ç»Ÿæ–¹æ³•ä¾èµ–è¦†ç›–ç‡: {traditional_coverage:.2f}")
        self.logger.info(f"ğŸ“Š è¯­ä¹‰åˆ†ææ–¹æ³•ä¾èµ–è¦†ç›–ç‡: {semantic_coverage:.2f}")
        
        # é€‰æ‹©è¦†ç›–ç‡æ›´é«˜çš„æ–¹æ³•
        if semantic_coverage > traditional_coverage:
            self.logger.info("âœ… é€‰æ‹©è¯­ä¹‰åˆ†ææ–¹æ³•ï¼ˆè¦†ç›–ç‡æ›´é«˜ï¼‰")
            return semantic_order
        else:
            self.logger.info("âœ… é€‰æ‹©ä¼ ç»Ÿæ–¹æ³•ï¼ˆè¦†ç›–ç‡æ›´é«˜ï¼‰")
            return traditional_order
    
    def _calculate_dependency_coverage(self, components: List[Dict[str, Any]], execution_order: List[str]) -> float:
        """è®¡ç®—æ‰§è¡Œé¡ºåºçš„ä¾èµ–è¦†ç›–ç‡"""
        if len(components) <= 1:
            return 1.0
        
        # ç»Ÿè®¡æ‰€æœ‰å¯èƒ½çš„ä¾èµ–å…³ç³»
        total_dependencies = 0
        satisfied_dependencies = 0
        
        for i, component in enumerate(components):
            params = component.get("params", {})
            placeholder_refs = self._extract_placeholder_references(params)
            
            for ref in placeholder_refs:
                total_dependencies += 1
                
                # æ£€æŸ¥ä¾èµ–æ˜¯å¦åœ¨æ‰§è¡Œé¡ºåºä¸­å¾—åˆ°æ»¡è¶³
                source_node_id = ref["node_id"]
                target_node_id = component["id"]
                
                try:
                    source_index = execution_order.index(source_node_id)
                    target_index = execution_order.index(target_node_id)
                    
                    if source_index < target_index:
                        satisfied_dependencies += 1
                except ValueError:
                    # å¦‚æœèŠ‚ç‚¹ä¸åœ¨æ‰§è¡Œé¡ºåºä¸­ï¼Œè·³è¿‡
                    pass
        
        return satisfied_dependencies / total_dependencies if total_dependencies > 0 else 1.0
    
    def validate_execution_order(self, components: List[Dict[str, Any]], execution_order: List[str]) -> List[str]:
        """
        éªŒè¯æ‰§è¡Œé¡ºåºçš„æ­£ç¡®æ€§
        
        Args:
            components: pipelineç»„ä»¶åˆ—è¡¨
            execution_order: æ‰§è¡Œé¡ºåº
            
        Returns:
            éªŒè¯é”™è¯¯åˆ—è¡¨
        """
        errors = []
        node_ids = {comp["id"] for comp in components}
        
        # æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨æ‰§è¡Œé¡ºåºä¸­
        for node_id in node_ids:
            if node_id not in execution_order:
                errors.append(f"èŠ‚ç‚¹ {node_id} ä¸åœ¨æ‰§è¡Œé¡ºåºä¸­")
        
        # æ£€æŸ¥æ‰§è¡Œé¡ºåºä¸­çš„èŠ‚ç‚¹éƒ½å­˜åœ¨
        for node_id in execution_order:
            if node_id not in node_ids:
                errors.append(f"æ‰§è¡Œé¡ºåºä¸­çš„èŠ‚ç‚¹ {node_id} ä¸å­˜åœ¨")
        
        # æ£€æŸ¥ä¾èµ–å…³ç³»æ˜¯å¦æ»¡è¶³
        dependencies = {}
        for component in components:
            node_id = component["id"]
            dependencies[node_id] = set()
            
            params = component.get("params", {})
            placeholder_refs = self._extract_placeholder_references(params)
            
            for ref in placeholder_refs:
                if ref["node_id"] in node_ids:
                    dependencies[node_id].add(ref["node_id"])
        
        # éªŒè¯æ¯ä¸ªèŠ‚ç‚¹çš„ä¾èµ–éƒ½åœ¨å…¶ä¹‹å‰æ‰§è¡Œ
        for i, node_id in enumerate(execution_order):
            node_deps = dependencies.get(node_id, set())
            for dep in node_deps:
                if dep in execution_order[:i]:
                    continue  # ä¾èµ–å·²æ»¡è¶³
                else:
                    errors.append(f"èŠ‚ç‚¹ {node_id} çš„ä¾èµ– {dep} åœ¨å…¶ä¹‹åæ‰§è¡Œ")
        
        return errors
    
    def create_node_output(self, node_id: str, output_def: Dict[str, Any], actual_output: Any) -> NodeOutput:
        """
        åˆ›å»ºèŠ‚ç‚¹è¾“å‡ºå¯¹è±¡
        
        Args:
            node_id: èŠ‚ç‚¹ID
            output_def: è¾“å‡ºå®šä¹‰
            actual_output: å®é™…è¾“å‡ºå€¼
            
        Returns:
            NodeOutputå¯¹è±¡
        """
        return NodeOutput(
            node_id=node_id,
            output_type=output_def.get("type", "any"),
            output_key=output_def.get("key", "output"),
            value=actual_output,
            description=output_def.get("description", "")
        )

# ä½¿ç”¨ç¤ºä¾‹
def demo_placeholder_resolution():
    """æ¼”ç¤ºå ä½ç¬¦è§£æåŠŸèƒ½"""
    resolver = PlaceholderResolver()
    
    # æ¨¡æ‹ŸèŠ‚ç‚¹è¾“å‡º
    node_outputs = {
        "rotate_node": NodeOutput(
            node_id="rotate_node",
            output_type="image_path",
            output_key="rotated_image",
            value="/path/to/rotated.jpg",
            description="æ—‹è½¬åçš„å›¾ç‰‡è·¯å¾„"
        ),
        "scale_node": NodeOutput(
            node_id="scale_node",
            output_type="image_path",
            output_key="scaled_image",
            value="/path/to/scaled.jpg",
            description="ç¼©æ”¾åçš„å›¾ç‰‡è·¯å¾„"
        ),
        "process_node": NodeOutput(
            node_id="process_node",
            output_type="json",
            output_key="result",
            value={"status": "success", "file_path": "/path/to/processed.jpg"},
            description="å¤„ç†ç»“æœ"
        )
    }
    
    # æµ‹è¯•å‚æ•°è§£æ
    test_params = {
        "image_path": "$rotate_node.output",
        "scale_factor": 2.0,
        "output_path": "$scale_node.output",
        "config": {
            "input_file": "$process_node.output.file_path",
            "status": "$process_node.output.status"
        }
    }
    
    resolved_params = resolver.resolve_placeholders(test_params, node_outputs)
    
    print("åŸå§‹å‚æ•°:", test_params)
    print("è§£æåå‚æ•°:", resolved_params)
    
    # æµ‹è¯•pipelineéªŒè¯
    test_components = [
        {
            "id": "rotate_node",
            "tool_type": "image_rotator",
            "params": {"image_path": "input.jpg"},
            "output": {"type": "image_path", "key": "rotated_image"}
        },
        {
            "id": "scale_node",
            "tool_type": "image_scaler",
            "params": {"image_path": "$rotate_node.output"},
            "output": {"type": "image_path", "key": "scaled_image"}
        },
        {
            "id": "upload_node",
            "tool_type": "file_uploader",
            "params": {"file_path": "$scale_node.output"},
            "output": {"type": "json", "key": "upload_result"}
        }
    ]
    
    errors = resolver.validate_pipeline_dependencies(test_components)
    if errors:
        print("PipelineéªŒè¯é”™è¯¯:", errors)
    else:
        print("PipelineéªŒè¯é€šè¿‡")
        
    execution_order = resolver.build_execution_order(test_components)
    print("æ‰§è¡Œé¡ºåº:", execution_order)

if __name__ == "__main__":
    demo_placeholder_resolution() 