#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå™¨
ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡ã€ä¸»é¢˜ç›¸å…³çš„æŠ¥å‘Šå†…å®¹
"""

import logging
import re
import time
import json
from typing import List, Dict, Any, Union, Optional
from tools.base_tool import create_standardized_output

logging.basicConfig(level=logging.INFO)

def enhanced_report_generator(
    content: Union[str, list, dict], 
    format: str = "markdown",
    max_words: int = 800,
    title: str = "æŠ¥å‘Š",
    topic: str = None,
    style: str = "professional"
) -> Dict[str, Any]:
    """
    ä¸“ä¸šæ•°æ®æŠ¥å‘Šç”Ÿæˆå·¥å…·
    
    ä¸“é—¨ç”¨äºå°†ç»“æ„åŒ–æ•°æ®ã€æœç´¢ç»“æœæˆ–å¤§é‡æ–‡æœ¬å†…å®¹ç”Ÿæˆä¸“ä¸šæŠ¥å‘Šã€‚
    é€‚ç”¨åœºæ™¯ï¼šæœç´¢ç»“æœæ±‡æ€»ã€æ•°æ®åˆ†ææŠ¥å‘Šã€ç ”ç©¶æŠ¥å‘Šã€æŠ€æœ¯æ–‡æ¡£ç­‰ã€‚
    
    âš ï¸ æ³¨æ„ï¼šæ­¤å·¥å…·ä»…é€‚ç”¨äºæœ‰å®è´¨å†…å®¹çš„æ•°æ®å¤„ç†ï¼Œä¸é€‚ç”¨äºç®€å•é—®å€™æˆ–é—²èŠã€‚
    
    å‚æ•°:
        content: å¾…å¤„ç†çš„æ•°æ®å†…å®¹ï¼ˆæœç´¢ç»“æœã€æ–‡æœ¬æ•°æ®ã€ç»“æ„åŒ–ä¿¡æ¯ç­‰ï¼‰
        format: è¾“å‡ºæ ¼å¼ï¼Œæ”¯æŒ "markdown" æˆ– "plain"
        max_words: æœ€å¤§å­—æ•°é™åˆ¶ï¼Œé»˜è®¤800å­—
        title: æŠ¥å‘Šæ ‡é¢˜ï¼Œé»˜è®¤"æŠ¥å‘Š"
        topic: ä¸»é¢˜å…³é”®è¯ï¼Œç”¨äºå†…å®¹èšç„¦
        style: æŠ¥å‘Šé£æ ¼ï¼Œæ”¯æŒ "professional", "academic", "casual"
        
    è¿”å›:
        æ ‡å‡†åŒ–çš„è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ç”Ÿæˆçš„ä¸“ä¸šæŠ¥å‘Šå†…å®¹
    """
    start_time = time.time()
    
    try:
        # å‚æ•°éªŒè¯
        if not content:
            return create_standardized_output(
                tool_name="enhanced_report_generator",
                status="error",
                message="è¾“å…¥å†…å®¹ä¸èƒ½ä¸ºç©º",
                error="è¾“å…¥å†…å®¹ä¸èƒ½ä¸ºç©º",
                start_time=start_time,
                parameters={"content": content, "format": format, "max_words": max_words}
            )
        
        # éªŒè¯å†…å®¹æ˜¯å¦é€‚åˆç”ŸæˆæŠ¥å‘Š
        content_str = str(content)
        if _is_casual_conversation(content_str):
            # æä¾›æ›´è¯¦ç»†å’Œå‹å¥½çš„é”™è¯¯è¯´æ˜
            error_msg = "è¯¥æŸ¥è¯¢æ›´é€‚åˆç›´æ¥å›ç­”ï¼Œæ— éœ€ç”Ÿæˆä¸“ä¸šæŠ¥å‘Šã€‚\n\n"
            error_msg += "ğŸ” enhanced_report_generator ä¸“é—¨ç”¨äºï¼š\n"
            error_msg += "â€¢ ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š\n"
            error_msg += "â€¢ æ•´ç†å¤§é‡æ•°æ®èµ„æ–™\n"
            error_msg += "â€¢ æ’°å†™ç ”ç©¶æ€»ç»“æ–‡æ¡£\n\n"
            error_msg += "ğŸ’¡ å¯¹äºæ‚¨çš„æŸ¥è¯¢ï¼Œå»ºè®®ï¼š\n"
            error_msg += "â€¢ ç®€å•ä¿¡æ¯æŸ¥è¯¢ â†’ ä½¿ç”¨æœç´¢å·¥å…·\n"
            error_msg += "â€¢ æ—¥å¸¸å¯¹è¯äº¤æµ â†’ ç›´æ¥å›ç­”å³å¯\n"
            error_msg += "â€¢ å¦‚éœ€ä¸“ä¸šæŠ¥å‘Š â†’ è¯·æ˜ç¡®è¯´æ˜æŠ¥å‘Šéœ€æ±‚"
            
            return create_standardized_output(
                tool_name="enhanced_report_generator",
                status="error",
                message=error_msg,
                error="å†…å®¹ä¸é€‚åˆç”Ÿæˆä¸“ä¸šæŠ¥å‘Š",
                start_time=start_time,
                parameters={"content": content, "format": format, "max_words": max_words}
            )
        
        if format not in ["markdown", "plain"]:
            return create_standardized_output(
                tool_name="enhanced_report_generator",
                status="error",
                message="æ ¼å¼å¿…é¡»æ˜¯ 'markdown' æˆ– 'plain'",
                error="ä¸æ”¯æŒçš„æ ¼å¼",
                start_time=start_time,
                parameters={"content": content, "format": format, "max_words": max_words}
            )
        
        # æå–å’Œæ¸…ç†å†…å®¹
        extracted_content = _extract_content(content)
        if not extracted_content:
            return create_standardized_output(
                tool_name="enhanced_report_generator",
                status="error",
                message="æ— æ³•ä»è¾“å…¥ä¸­æå–æœ‰æ•ˆå†…å®¹",
                error="å†…å®¹æå–å¤±è´¥",
                start_time=start_time,
                parameters={"content": content, "format": format, "max_words": max_words}
            )
        
        # æ™ºèƒ½åˆ†æå†…å®¹
        analysis_result = _smart_analyze_content(extracted_content, topic)
        
        # ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Š
        report_content = _generate_llm_report(
            extracted_content, analysis_result, title, max_words, format, style
        )
        
        # æ„å»ºè¾“å‡º
        output_data = {
            "report_content": report_content,
            "format": format,
            "word_count": len(report_content.split()),
            "content_length": len(report_content),
            "analysis": analysis_result,
            "topic": topic,
            "style": style
        }
        
        return create_standardized_output(
            tool_name="enhanced_report_generator",
            status="success",
            primary_data=output_data,
            secondary_data={
                "original_content_length": len(extracted_content),
                "extraction_method": _get_extraction_method(content)
            },
            counts={
                "report_words": len(report_content.split()),
                "report_chars": len(report_content),
                "original_words": len(extracted_content.split()),
                "original_chars": len(extracted_content)
            },
            message=f"æˆåŠŸç”Ÿæˆ{format}æ ¼å¼æŠ¥å‘Šï¼Œå­—æ•°: {len(report_content.split())}",
            start_time=start_time,
            parameters={"content": content, "format": format, "max_words": max_words, "title": title, "topic": topic, "style": style}
        )

    except Exception as e:
        logging.error(f"å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return create_standardized_output(
            tool_name="enhanced_report_generator",
            status="error",
            message=f"å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}",
            error=str(e),
            start_time=start_time,
            parameters={"content": content, "format": format, "max_words": max_words}
        )

def _is_casual_conversation(content: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºä¸é€‚åˆç”Ÿæˆä¸“ä¸šæŠ¥å‘Šçš„å†…å®¹ï¼ŒåŒ…æ‹¬é—²èŠå’Œç®€å•çŸ¥è¯†æŸ¥è¯¢"""
    if not content or len(content.strip()) < 3:
        return True
    
    content = content.strip().lower()
    
    # 1. å¸¸è§é—²èŠæ¨¡å¼
    casual_patterns = [
        r'^(ä½ å¥½|hi|hello|å—¨|æ‚¨å¥½)',
        r'^(è°¢è°¢|thanks|thx|æ„Ÿè°¢)',
        r'^(å†è§|bye|æ‹œæ‹œ|goodbye)',
        r'^(æ—©ä¸Šå¥½|ä¸‹åˆå¥½|æ™šä¸Šå¥½|ä¸­åˆå¥½)',
        r'^åƒäº†å—',
        r'^æ‚¨é‚£.*æ€ä¹ˆæ ·',
        r'^æœ€è¿‘.*å¦‚ä½•',
        r'^èº«ä½“.*æ€ä¹ˆæ ·',
        r'^å·¥ä½œ.*é¡ºåˆ©',
        r'^å¤©æ°”.*ä¸é”™',
        r'^å¿™ä¸å¿™',
        r'^ä¼‘æ¯.*æ²¡æœ‰',
        r'^ç¡å¾—.*å¥½',
        r'^æœ‰ç©º.*èŠèŠ',
        r'^(å¥½çš„|ok|æ˜¯çš„|ä¸æ˜¯|è¡Œ|ä¸è¡Œ)$',
        r'^æ€ä¹ˆæ ·$',
        r'^å¦‚ä½•$',
        r'^è¿˜å¥½å—$',
    ]
    
    # 2. ç®€å•çŸ¥è¯†æŸ¥è¯¢æ¨¡å¼ï¼ˆä¸é€‚åˆç”Ÿæˆä¸“ä¸šæŠ¥å‘Šï¼‰
    simple_query_patterns = [
        r'^[^ï¼Œã€‚ï¼ï¼Ÿ]{1,8}$',  # éå¸¸ç®€çŸ­çš„æŸ¥è¯¢ï¼ˆå¦‚"ç§¦å§‹çš‡ä¹‹æ­»"ã€"è¯¸è‘›äº®"ï¼‰
        r'^(è°æ˜¯|ä»€ä¹ˆæ˜¯|å“ªé‡Œæ˜¯)[^ï¼Œã€‚ï¼ï¼Ÿ]{1,10}$',  # ç®€å•çš„"è°æ˜¯XX"ã€"ä»€ä¹ˆæ˜¯XX"
        r'^[^ï¼Œã€‚ï¼ï¼Ÿ]{1,6}(æ˜¯è°|æ˜¯ä»€ä¹ˆ|æ€ä¹ˆæ­»çš„|ä¹‹æ­»)$',  # "XXæ˜¯è°"ã€"XXä¹‹æ­»"ç­‰
        r'^[^ï¼Œã€‚ï¼ï¼Ÿ]{1,10}(ä»‹ç»|ç®€ä»‹)$',  # "XXä»‹ç»"
        r'^(å†å²ä¸Šçš„|å¤ä»£çš„)[^ï¼Œã€‚ï¼ï¼Ÿ]{1,8}$',  # "å†å²ä¸Šçš„XX"
        r'^[^ï¼Œã€‚ï¼ï¼Ÿ]{1,8}(ç”Ÿå¹³|ç»å†)$',  # "XXç”Ÿå¹³"
    ]
    
    # 3. å•çº¯çš„äººåã€åœ°åã€æ¦‚å¿µåï¼ˆä¸é€‚åˆæŠ¥å‘Šæ ¼å¼ï¼‰
    single_entity_patterns = [
        r'^[ä¸€-é¾¥]{2,4}$',  # 2-4ä¸ªæ±‰å­—ï¼ˆå¸¸è§äººåï¼‰
        r'^[ä¸€-é¾¥]{2,6}(å¸|ç‹|å…¬|ä¾¯|å°†å†›|ä¸ç›¸)$',  # å†å²äººç‰©ç§°è°“
        r'^(æ˜¥ç§‹|æˆ˜å›½|ç§¦æœ|æ±‰æœ|å”æœ|å®‹æœ|æ˜æœ|æ¸…æœ)[ä¸€-é¾¥]{0,4}$',  # æœä»£ç›¸å…³
    ]
    
    # æ£€æŸ¥æ‰€æœ‰æ¨¡å¼
    all_patterns = casual_patterns + simple_query_patterns + single_entity_patterns
    
    for pattern in all_patterns:
        if re.search(pattern, content):
            return True
    
    # 4. å†…å®¹é•¿åº¦å’Œå¤æ‚åº¦åˆ¤æ–­
    # å¦‚æœå†…å®¹å¾ˆçŸ­ä¸”æ²¡æœ‰æ˜ç¡®çš„æŠ¥å‘Šéœ€æ±‚å…³é”®è¯ï¼Œä¸é€‚åˆç”ŸæˆæŠ¥å‘Š
    if len(content) < 15:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜ç¡®çš„æŠ¥å‘Šéœ€æ±‚å…³é”®è¯
        report_indicators = ['æŠ¥å‘Š', 'åˆ†æ', 'æ€»ç»“', 'ç ”ç©¶', 'è¯¦ç»†ä»‹ç»', 'æ·±å…¥äº†è§£', 'å…¨é¢åˆ†æ']
        if not any(indicator in content for indicator in report_indicators):
            return True
    
    # 5. æ£€æŸ¥æ˜¯å¦æ˜¯æ˜ç¡®è¦æ±‚æœç´¢è€ŒéæŠ¥å‘Šçš„å†…å®¹
    search_only_patterns = [
        r'(æœç´¢|æŸ¥æ‰¾|æ‰¾|æŸ¥|æŸ¥ä¸€ä¸‹|æœä¸€ä¸‹)[^ï¼Œã€‚ï¼ï¼Ÿ]{1,10}$',
        r'^[^ï¼Œã€‚ï¼ï¼Ÿ]{1,10}(çš„ä¿¡æ¯|çš„èµ„æ–™)$',
    ]
    
    for pattern in search_only_patterns:
        if re.search(pattern, content):
            return True
    
    return False

def _extract_content(content: Union[str, list, dict]) -> str:
    """ä»å„ç§è¾“å…¥æ ¼å¼ä¸­æå–æ–‡æœ¬å†…å®¹"""
    if isinstance(content, str):
        return content.strip()
    
    elif isinstance(content, list):
        content_parts = []
        for item in content:
            if isinstance(item, dict):
                text = _extract_text_from_dict(item)
                if text:
                    content_parts.append(text)
            elif isinstance(item, str):
                content_parts.append(item)
        return '\n\n'.join(content_parts)
    
    elif isinstance(content, dict):
        return _extract_text_from_dict(content)
    
    else:
        return str(content)

def _extract_text_from_dict(data_dict: dict) -> str:
    """ä»å­—å…¸ä¸­æå–æ–‡æœ¬å†…å®¹"""
    # ä¼˜å…ˆæå–çš„å­—æ®µ - è°ƒæ•´ä¼˜å…ˆçº§ï¼Œfull_contentåº”è¯¥ä¼˜å…ˆäºsnippet
    priority_fields = ['full_content', 'content', 'text', 'body', 'description', 'summary', 'results', 'snippet', 'title']
    
    for field in priority_fields:
        if field in data_dict:
            value = data_dict[field]
            if isinstance(value, str) and value.strip():
                return value.strip()
            elif isinstance(value, list):
                return _extract_content(value)
            elif isinstance(value, dict):
                nested_text = _extract_text_from_dict(value)
                if nested_text:
                    return nested_text
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆå­—æ®µï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²å€¼
    text_parts = []
    for key, value in data_dict.items():
        if isinstance(value, str) and value.strip():
            text_parts.append(value.strip())
    
    return '\n'.join(text_parts)

def _smart_analyze_content(content: str, topic: str = None) -> Dict[str, Any]:
    """æ™ºèƒ½åˆ†æå†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯"""
    analysis = {
        "content_length": len(content),
        "word_count": len(content.split()),
        "language": _detect_language(content),
        "content_type": _identify_content_type(content),
        "key_entities": _extract_key_entities(content, topic),
        "main_topics": _identify_main_topics(content, topic),
        "summary": _generate_smart_summary(content, topic),
        "analysis_time": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    return analysis

def _detect_language(content: str) -> str:
    """æ£€æµ‹å†…å®¹è¯­è¨€"""
    # ç®€å•çš„è¯­è¨€æ£€æµ‹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
    english_chars = len(re.findall(r'[a-zA-Z]', content))
    
    if chinese_chars > english_chars:
        return "zh"
    else:
        return "en"

def _identify_content_type(content: str) -> str:
    """è¯†åˆ«å†…å®¹ç±»å‹"""
    content_lower = content.lower()
    
    if any(word in content_lower for word in ['æœç´¢', 'æŸ¥è¯¢', 'ç»“æœ', 'search', 'query']):
        return "search_results"
    elif any(word in content_lower for word in ['æ–°é—»', 'æŠ¥é“', 'news', 'report']):
        return "news"
    elif any(word in content_lower for word in ['æŠ€æœ¯', 'ä»£ç ', 'technical', 'code']):
        return "technical"
    else:
        return "general"

def _extract_key_entities(content: str, topic: str = None) -> List[str]:
    """æå–å…³é”®å®ä½“ï¼Œä¼˜å…ˆå…³æ³¨ä¸»é¢˜ç›¸å…³å®ä½“"""
    entities = []
    
    # å¦‚æœæŒ‡å®šäº†ä¸»é¢˜ï¼Œä¼˜å…ˆæå–ä¸»é¢˜ç›¸å…³çš„å®ä½“
    if topic:
        topic_entities = re.findall(rf'{topic}[^ï¼Œã€‚ï¼ï¼Ÿ\s]*', content)
        entities.extend(topic_entities[:5])
    
    # æå–äººåã€åœ°åã€æœºæ„åç­‰
    # äººåæ¨¡å¼
    name_patterns = [
        r'[A-Z][a-z]+ [A-Z][a-z]+',  # è‹±æ–‡å
        r'[\u4e00-\u9fff]{2,4}',     # ä¸­æ–‡å
    ]
    
    for pattern in name_patterns:
        matches = re.findall(pattern, content)
        entities.extend(matches[:3])
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_entities = list(set(entities))
    return unique_entities[:10]

def _identify_main_topics(content: str, topic: str = None) -> List[str]:
    """è¯†åˆ«ä¸»è¦è¯é¢˜ï¼Œä¼˜å…ˆå…³æ³¨ä¸»é¢˜ç›¸å…³å†…å®¹"""
    topics = []
    
    # å¦‚æœæŒ‡å®šäº†ä¸»é¢˜ï¼Œå°†å…¶ä½œä¸ºä¸»è¦è¯é¢˜
    if topic:
        topics.append(topic)
    
    # æ ¹æ®å†…å®¹ç±»å‹è¯†åˆ«è¯é¢˜
    content_lower = content.lower()
    
    topic_keywords = {
        "å†å²": ["å†å²", "å¤ä»£", "æœä»£", "çš‡å¸", "å†å²", "history"],
        "æ–‡åŒ–": ["æ–‡åŒ–", "ä¼ ç»Ÿ", "è‰ºæœ¯", "æ–‡å­¦", "culture"],
        "æ”¿æ²»": ["æ”¿æ²»", "æ”¿åºœ", "æ”¿ç­–", "æ”¿æ²»", "politics"],
        "å†›äº‹": ["å†›äº‹", "æˆ˜äº‰", "å†›é˜Ÿ", "å†›äº‹", "military"],
        "ç§‘æŠ€": ["ç§‘æŠ€", "æŠ€æœ¯", "ç§‘å­¦", "ç§‘æŠ€", "technology"],
        "åœ°ç†": ["åœ°ç†", "åœ°åŒº", "åœ°æ–¹", "åœ°ç†", "geography"]
    }
    
    for topic_name, keywords in topic_keywords.items():
        if any(keyword in content_lower for keyword in keywords):
            topics.append(topic_name)
    
    return topics[:5]

def _generate_smart_summary(content: str, topic: str = None) -> str:
    """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼Œèšç„¦ä¸»é¢˜ç›¸å…³å†…å®¹"""
    # åˆ†å¥
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # å¦‚æœæŒ‡å®šäº†ä¸»é¢˜ï¼Œä¼˜å…ˆé€‰æ‹©åŒ…å«ä¸»é¢˜çš„å¥å­
    if topic:
        topic_sentences = [s for s in sentences if topic in s]
        if topic_sentences:
            sentences = topic_sentences + [s for s in sentences if topic not in s]
    
    # é€‰æ‹©æœ€é‡è¦çš„å¥å­
    important_sentences = sentences[:3]
    
    if important_sentences:
        summary = ' '.join(important_sentences)
        if not summary.endswith('ã€‚'):
            summary += 'ã€‚'
        return summary
    else:
        return content[:200] + "..." if len(content) > 200 else content

def _generate_llm_report(
    content: str, 
    analysis: Dict[str, Any], 
    title: str, 
    max_words: int, 
    format: str,
    style: str
) -> str:
    """ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Š"""
    
    # æ„å»ºLLMæç¤ºè¯
    prompt = _build_llm_prompt(content, analysis, title, max_words, format, style)
    
    # å°è¯•è°ƒç”¨LLM
    try:
        return _call_llm_api(prompt)
    except Exception as e:
        logging.warning(f"LLM APIè°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°åŸºäºè§„åˆ™çš„æ–¹æ³•
        return _generate_fallback_report(content, analysis, title, max_words, format, style)

def _build_llm_prompt(
    content: str, 
    analysis: Dict[str, Any], 
    title: str, 
    max_words: int, 
    format: str,
    style: str
) -> str:
    """æ„å»ºLLMæç¤ºè¯"""
    
    style_guide = {
        "professional": "ä¸“ä¸šã€å®¢è§‚ã€ç®€æ´",
        "academic": "å­¦æœ¯ã€ä¸¥è°¨ã€è¯¦ç»†",
        "casual": "é€šä¿—ã€æ˜“æ‡‚ã€å‹å¥½"
    }
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ¥å‘Šç”Ÿæˆä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä»½é«˜è´¨é‡çš„{format}æ ¼å¼æŠ¥å‘Šã€‚

æŠ¥å‘Šè¦æ±‚ï¼š
- æ ‡é¢˜ï¼š{title}
- é£æ ¼ï¼š{style_guide.get(style, 'ä¸“ä¸š')}
- å­—æ•°é™åˆ¶ï¼š{max_words}å­—ä»¥å†…
- æ ¼å¼ï¼š{format.upper()}

å†…å®¹ä¿¡æ¯ï¼š
- ä¸»é¢˜ï¼š{analysis.get('main_topics', ['ä¸€èˆ¬ä¿¡æ¯'])}
- å…³é”®å®ä½“ï¼š{', '.join(analysis.get('key_entities', [])[:5])}
- å†…å®¹ç±»å‹ï¼š{analysis.get('content_type', 'general')}
- è¯­è¨€ï¼š{analysis.get('language', 'zh')}

åŸå§‹å†…å®¹ï¼š
{content[:2000]}  # é™åˆ¶é•¿åº¦

é‡è¦è¦æ±‚ï¼š
1. å†…å®¹å¿…é¡»ä¸ä¸»é¢˜é«˜åº¦ç›¸å…³ï¼Œé¿å…æ— å…³çš„æ¨¡æ¿åŒ–å†…å®¹
2. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘åˆç†
3. ä¿¡æ¯å‡†ç¡®ï¼Œé¿å…è™šæ„
4. è¯­è¨€æµç•…ï¼Œç¬¦åˆ{style}é£æ ¼
5. çªå‡ºé‡ç‚¹ï¼Œé¿å…å†—ä½™

è¯·ç”ŸæˆæŠ¥å‘Šï¼š
"""
    
    return prompt

def _call_llm_api(prompt: str) -> str:
    """è°ƒç”¨LLM API"""
    try:
        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§LLM API
        # ä¾‹å¦‚ï¼šOpenAI GPTã€ç™¾åº¦æ–‡å¿ƒä¸€è¨€ã€é˜¿é‡Œé€šä¹‰åƒé—®ç­‰
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨OpenAI API
        # import openai
        # client = openai.OpenAI(api_key="your-api-key")
        # response = client.chat.completions.create(
        #     model="gpt-4",
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=2000,
        #     temperature=0.7
        # )
        # return response.choices[0].message.content
        
        # ä¸´æ—¶è¿”å›åŸºäºè§„åˆ™çš„å†…å®¹
        return _generate_fallback_report_simple(prompt)
        
    except Exception as e:
        logging.error(f"LLM APIè°ƒç”¨å¤±è´¥: {e}")
        raise e

def _generate_fallback_report(
    content: str, 
    analysis: Dict[str, Any], 
    title: str, 
    max_words: int, 
    format: str,
    style: str
) -> str:
    """ç”Ÿæˆå›é€€æŠ¥å‘Š"""
    
    if format == "markdown":
        return _generate_markdown_fallback(content, analysis, title, max_words, style)
    else:
        return _generate_plain_fallback(content, analysis, title, max_words, style)

def _generate_fallback_report_simple(prompt: str) -> str:
    """ç®€å•çš„å›é€€æŠ¥å‘Šç”Ÿæˆ"""
    # ä»promptä¸­æå–å…³é”®ä¿¡æ¯
    lines = prompt.split('\n')
    
    # æå–æ ‡é¢˜
    title = "æŠ¥å‘Š"
    for line in lines:
        if "æ ‡é¢˜ï¼š" in line:
            title = line.split("ï¼š")[1].strip()
            break
    
    # æå–ä¸»é¢˜
    topics = []
    for line in lines:
        if "ä¸»é¢˜ï¼š" in line:
            topic_part = line.split("ï¼š")[1].strip()
            topics = [t.strip() for t in topic_part.strip('[]').split(',')]
            break
    
    # ç”Ÿæˆç®€å•æŠ¥å‘Š
    report = f"# {title}\n\n"
    
    if topics:
        report += f"## æ¦‚è¿°\n\n"
        report += f"æœ¬æŠ¥å‘Šä¸»è¦å…³æ³¨{', '.join(topics)}ç›¸å…³å†…å®¹ã€‚\n\n"
    
    report += "## ä¸»è¦å†…å®¹\n\n"
    report += "åŸºäºæä¾›çš„åŸå§‹å†…å®¹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæå–äº†å…³é”®ä¿¡æ¯ã€‚\n\n"
    
    report += "## å…³é”®å‘ç°\n\n"
    report += "1. å†…å®¹ä¿¡æ¯ä¸°å¯Œï¼Œå…·æœ‰é‡è¦å‚è€ƒä»·å€¼\n"
    report += "2. æ¶‰åŠå¤šä¸ªç›¸å…³é¢†åŸŸï¼Œè¦†ç›–é¢å¹¿\n"
    report += "3. ä¿¡æ¯å‡†ç¡®å¯é ï¼Œé€‚åˆè¿›ä¸€æ­¥ç ”ç©¶\n\n"
    
    report += "## ç»“è®º\n\n"
    report += "æœ¬æŠ¥å‘Šä¸ºç›¸å…³ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦åŸºç¡€ä¿¡æ¯ã€‚"
    
    return report

def _generate_markdown_fallback(
    content: str, 
    analysis: Dict[str, Any], 
    title: str, 
    max_words: int,
    style: str
) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼çš„å›é€€æŠ¥å‘Š"""
    
    report_parts = []
    
    # æ ‡é¢˜
    report_parts.append(f"# {title}\n")
    
    # æ¦‚è¿°
    topics = analysis.get("main_topics", [])
    if topics:
        report_parts.append("## æ¦‚è¿°\n")
        report_parts.append(f"æœ¬æŠ¥å‘Šä¸»è¦å…³æ³¨{', '.join(topics)}ç›¸å…³å†…å®¹ã€‚\n")
    
    # ä¸»è¦å†…å®¹
    report_parts.append("## ä¸»è¦å†…å®¹\n")
    summary = analysis.get("summary", "")
    if summary:
        report_parts.append(f"{summary}\n")
    
    # å…³é”®ä¿¡æ¯
    entities = analysis.get("key_entities", [])
    if entities:
        report_parts.append("## å…³é”®ä¿¡æ¯\n")
        for entity in entities[:5]:
            report_parts.append(f"- {entity}")
        report_parts.append("")
    
    # ç»“è®º
    report_parts.append("## ç»“è®º\n")
    report_parts.append("åŸºäºå¯¹åŸå§‹å†…å®¹çš„åˆ†æï¼Œæœ¬æŠ¥å‘Šæä¾›äº†é‡è¦çš„å‚è€ƒä¿¡æ¯ã€‚")
    
    report = '\n'.join(report_parts)
    
    # æ§åˆ¶å­—æ•°
    if len(report.split()) > max_words:
        report = _truncate_report(report, max_words)
    
    return report

def _generate_plain_fallback(
    content: str, 
    analysis: Dict[str, Any], 
    title: str, 
    max_words: int,
    style: str
) -> str:
    """ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼çš„å›é€€æŠ¥å‘Š"""
    
    report_parts = []
    
    # æ ‡é¢˜
    report_parts.append(f"{title}")
    report_parts.append("=" * len(title))
    report_parts.append("")
    
    # æ¦‚è¿°
    topics = analysis.get("main_topics", [])
    if topics:
        report_parts.append("æ¦‚è¿°")
        report_parts.append("-" * len("æ¦‚è¿°"))
        report_parts.append(f"æœ¬æŠ¥å‘Šä¸»è¦å…³æ³¨{', '.join(topics)}ç›¸å…³å†…å®¹ã€‚")
        report_parts.append("")
    
    # ä¸»è¦å†…å®¹
    report_parts.append("ä¸»è¦å†…å®¹")
    report_parts.append("-" * len("ä¸»è¦å†…å®¹"))
    summary = analysis.get("summary", "")
    if summary:
        report_parts.append(summary)
    report_parts.append("")
    
    # ç»“è®º
    report_parts.append("ç»“è®º")
    report_parts.append("-" * len("ç»“è®º"))
    report_parts.append("åŸºäºå¯¹åŸå§‹å†…å®¹çš„åˆ†æï¼Œæœ¬æŠ¥å‘Šæä¾›äº†é‡è¦çš„å‚è€ƒä¿¡æ¯ã€‚")
    
    report = '\n'.join(report_parts)
    
    # æ§åˆ¶å­—æ•°
    if len(report.split()) > max_words:
        report = _truncate_report(report, max_words)
    
    return report

def _truncate_report(report: str, max_words: int) -> str:
    """æˆªæ–­æŠ¥å‘Šä»¥æ§åˆ¶å­—æ•°"""
    words = report.split()
    if len(words) <= max_words:
        return report
    
    # ä¿ç•™å‰é¢çš„éƒ¨åˆ†
    truncated_words = words[:max_words]
    report = ' '.join(truncated_words)
    
    # ç¡®ä¿å¥å­å®Œæ•´
    if not report.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
        last_sentence_end = max(
            report.rfind('.'), report.rfind('ã€‚'),
            report.rfind('!'), report.rfind('ï¼'),
            report.rfind('?'), report.rfind('ï¼Ÿ')
        )
        if last_sentence_end > 0:
            report = report[:last_sentence_end + 1]
    
    return report

def _get_extraction_method(content: Union[str, list, dict]) -> str:
    """è·å–å†…å®¹æå–æ–¹æ³•"""
    if isinstance(content, str):
        return "direct_string"
    elif isinstance(content, list):
        return "list_extraction"
    elif isinstance(content, dict):
        return "dict_extraction"
    else:
        return "unknown" 