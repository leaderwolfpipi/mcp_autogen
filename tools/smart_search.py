import logging
import requests
import re
import time
from typing import List, Dict, Any, Union
from bs4 import BeautifulSoup
from tools.base_tool import create_standardized_output

# å¯¼å…¥ç°æœ‰çš„æœç´¢å·¥å…·
from .search_tool import search_tool

def smart_search(query: str, max_results: int = 3) -> Dict[str, Any]:
    """
    ğŸ¯ æ¨èæœç´¢å·¥å…· - å¤šå¼•æ“æ™ºèƒ½æœç´¢ï¼ˆè°·æ­Œ+ç™¾åº¦ï¼‰
    
    è¿™æ˜¯æœ€ç¨³å®šå¯é çš„æœç´¢å·¥å…·ï¼Œè‡ªåŠ¨å¤„ç†æœç´¢å¼•æ“æ•…éšœå’Œç½‘ç»œé—®é¢˜ã€‚
    æ”¯æŒå¤šæœç´¢å¼•æ“å›é€€æœºåˆ¶ï¼Œæ— éœ€é¢å¤–ä¾èµ–åº“ï¼Œè·å–å®Œæ•´ç½‘é¡µå†…å®¹ã€‚
    
    ğŸ”¥ ä¼˜åŠ¿ç‰¹æ€§ï¼š
    - å¤šå¼•æ“æ”¯æŒï¼šè°·æ­Œæœç´¢ + ç™¾åº¦æœç´¢è‡ªåŠ¨å›é€€
    - é›¶ä¾èµ–ï¼šæ— éœ€å¤–éƒ¨åº“ï¼Œè‡ªå¸¦ç½‘ç»œè¯·æ±‚å¤„ç†
    - å†…å®¹å¢å¼ºï¼šè‡ªåŠ¨è·å–å®Œæ•´ç½‘é¡µå†…å®¹å¹¶æ™ºèƒ½å‡€åŒ–
    - é«˜å¯ç”¨æ€§ï¼šè‡ªåŠ¨å¤„ç†æœç´¢å¼•æ“æ•…éšœå’Œè¶…æ—¶
    - æ ‡å‡†åŒ–è¾“å‡ºï¼šå®Œå…¨ç¬¦åˆMCPåè®®æ ‡å‡†
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - é€šç”¨æœç´¢æŸ¥è¯¢ï¼ˆæ¨èé¦–é€‰ï¼‰
    - éœ€è¦ç¨³å®šå¯é çš„æœç´¢æœåŠ¡
    - éœ€è¦å®Œæ•´ç½‘é¡µå†…å®¹çš„æ·±åº¦æœç´¢
    - è·¨åœ°åŸŸæœç´¢éœ€æ±‚

    å‚æ•°ï¼š
        query (str): æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸èƒ½ä¸ºç©ºã€‚
        max_results (int): æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤3ä¸ªï¼ŒèŒƒå›´1-10ã€‚
    è¿”å›ï¼š
        dict: æ ‡å‡†åŒ–è¾“å‡ºï¼Œå­—æ®µåŒ…æ‹¬ï¼š
            status: 'success' | 'error'
            data.primary: å¢å¼ºæœç´¢ç»“æœåˆ—è¡¨ï¼ˆåŒ…å«å®Œæ•´å†…å®¹ï¼‰
            data.secondary: æœç´¢å’Œå†…å®¹è·å–å…ƒä¿¡æ¯
            data.counts: ç»“æœç»Ÿè®¡
            metadata: å·¥å…·å…ƒä¿¡æ¯
            paths: æœç´¢æºä¿¡æ¯
            message: æ‰§è¡Œæ¶ˆæ¯
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
    """
    logger = logging.getLogger("smart_search")
    start_time = time.time()
    
    try:
        logger.info(f"å¼€å§‹æ™ºèƒ½æœç´¢: {query}")
        
        # å‚æ•°éªŒè¯
        if not query or not query.strip():
            return create_standardized_output(
                tool_name="smart_search",
                status="error",
                message="æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º",
                error="æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º",
                start_time=start_time,
                parameters={"query": query, "max_results": max_results}
            )
        
        if not isinstance(max_results, int) or max_results < 1 or max_results > 10:
            max_results = 3
            logger.warning(f"max_resultså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: {max_results}")
        
        # ä½¿ç”¨ç°æœ‰çš„æœç´¢å·¥å…·è·å–åŸºç¡€æœç´¢ç»“æœ
        search_result = search_tool(query, max_results)
        
        if search_result.get("status") == "success":
            results = search_result.get("data", {}).get("primary", [])
            logger.info(f"åŸºç¡€æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            
            # å¢å¼ºæœç´¢ç»“æœï¼Œè·å–æ›´å®Œæ•´çš„å†…å®¹
            enhanced_results = []
            content_stats = {
                "total_processed": 0,
                "successful_content_fetch": 0,
                "failed_content_fetch": 0,
                "total_content_length": 0
            }
            
            for i, result in enumerate(results):
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªç»“æœ: {result.get('title', 'Unknown')}")
                content_stats["total_processed"] += 1
                
                enhanced_result = result.copy()
                
                # å°è¯•è·å–æ›´å®Œæ•´çš„å†…å®¹
                try:
                    full_content = _fetch_full_content(result.get('link', ''))
                    if full_content:
                        enhanced_result['full_content'] = full_content
                        enhanced_result['content_length'] = len(full_content)
                        content_stats["successful_content_fetch"] += 1
                        content_stats["total_content_length"] += len(full_content)
                        logger.info(f"æˆåŠŸè·å–å®Œæ•´å†…å®¹ï¼Œé•¿åº¦: {len(full_content)} å­—ç¬¦")
                    else:
                        enhanced_result['full_content'] = result.get('snippet', '')
                        enhanced_result['content_length'] = len(result.get('snippet', ''))
                        content_stats["failed_content_fetch"] += 1
                        logger.info(f"æ— æ³•è·å–å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨æ‘˜è¦")
                except Exception as e:
                    logger.warning(f"è·å–å®Œæ•´å†…å®¹å¤±è´¥: {e}")
                    enhanced_result['full_content'] = result.get('snippet', '')
                    enhanced_result['content_length'] = len(result.get('snippet', ''))
                    content_stats["failed_content_fetch"] += 1
                
                enhanced_results.append(enhanced_result)
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                if i < len(results) - 1:
                    time.sleep(0.5)
            
            logger.info(f"æ™ºèƒ½æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(enhanced_results)} ä¸ªå¢å¼ºç»“æœ")
            
            # æ„å»ºæ ‡å‡†åŒ–è¾“å‡º
            return create_standardized_output(
                tool_name="smart_search",
                status="success",
                primary_data=enhanced_results,
                secondary_data={
                    "content_stats": content_stats,
                    "base_search_result": search_result,
                    "query": query
                },
                counts={
                    "total": len(enhanced_results),
                    "requested": max_results,
                    "actual": len(enhanced_results),
                    "content_fetch_success": content_stats["successful_content_fetch"],
                    "content_fetch_failed": content_stats["failed_content_fetch"]
                },
                paths=search_result.get("paths", []),
                message=f"æ™ºèƒ½æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(enhanced_results)} ä¸ªå¢å¼ºç»“æœï¼Œå†…å®¹è·å–æˆåŠŸç‡: {content_stats['successful_content_fetch']}/{content_stats['total_processed']}",
                start_time=start_time,
                parameters={"query": query, "max_results": max_results}
            )
        else:
            return create_standardized_output(
                tool_name="smart_search",
                status="error",
                message=f"åŸºç¡€æœç´¢å¤±è´¥: {search_result.get('message', 'æœªçŸ¥é”™è¯¯')}",
                error=search_result.get('message', 'æœªçŸ¥é”™è¯¯'),
                start_time=start_time,
                parameters={"query": query, "max_results": max_results}
            )
            
    except Exception as e:
        logger.error(f"æ™ºèƒ½æœç´¢æ‰§è¡Œå¤±è´¥: {e}")
        return create_standardized_output(
            tool_name="smart_search",
            status="error",
            message=f"æ™ºèƒ½æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}",
            error=str(e),
            start_time=start_time,
            parameters={"query": query, "max_results": max_results}
        )

def _fetch_full_content(url: str, timeout: int = 10) -> str:
    """
    è·å–ç½‘é¡µçš„å®Œæ•´å†…å®¹å¹¶è¿›è¡Œæ™ºèƒ½å‡€åŒ–
    
    Args:
        url: ç½‘é¡µURL
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        
    Returns:
        å‡€åŒ–åçš„ç½‘é¡µæ–‡æœ¬å†…å®¹
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(["script", "style", "nav", "header", "footer", "aside", "menu", "sidebar"]):
            element.decompose()
        
        # ç§»é™¤å¸¸è§çš„å¯¼èˆªå’Œå¹¿å‘Šå…ƒç´ 
        for element in soup.find_all(["div", "span"], class_=lambda x: x and any(keyword in x.lower() for keyword in [
            "nav", "menu", "header", "footer", "sidebar", "ad", "banner", "toolbar", "breadcrumb",
            "comment", "share", "social", "related", "recommend", "hot", "trending"
        ])):
            element.decompose()
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = None
        
        # å¸¸è§çš„æ­£æ–‡å®¹å™¨
        content_selectors = [
            "main", "article", ".content", ".main-content", ".post-content", 
            ".entry-content", ".article-content", "#content", "#main",
            ".mw-parser-output", ".content-body", ".text-content",
            ".article", ".post", ".story", ".news-content"
        ]
        
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
        if not main_content:
            main_content = soup.find("body") or soup
        
        # æå–æ–‡æœ¬å†…å®¹
        text = main_content.get_text()
        
        # æ™ºèƒ½å‡€åŒ–æ–‡æœ¬å†…å®¹
        text = _clean_and_filter_content(text)
        
        return text
        
    except Exception as e:
        logging.warning(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥ {url}: {e}")
        return ""

def _clean_and_filter_content(text: str) -> str:
    """
    æ™ºèƒ½å‡€åŒ–å’Œè¿‡æ»¤æ–‡æœ¬å†…å®¹
    
    Args:
        text: åŸå§‹æ–‡æœ¬å†…å®¹
        
    Returns:
        å‡€åŒ–åçš„æ–‡æœ¬å†…å®¹
    """
    if not text:
        return ""
    
    # 1. åŸºç¡€æ¸…ç†
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 2. ç§»é™¤æ— ç”¨çš„å¯¼èˆªå’Œç•Œé¢å…ƒç´ 
    useless_patterns = [
        r'è·³è½¬åˆ°å†…å®¹.*?ä¸»èœå•.*?ç§»è‡³ä¾§æ .*?éšè—.*?å¯¼èˆª',
        r'é¦–é¡µ.*?åˆ†ç±».*?ç´¢å¼•.*?ç‰¹è‰²å†…å®¹.*?æ–°é—»åŠ¨æ€.*?æœ€è¿‘æ›´æ”¹',
        r'éšæœºæ¡ç›®.*?å¸®åŠ©.*?ç»´åŸºç¤¾ç¾¤.*?æ–¹é’ˆä¸æŒ‡å¼•.*?äº’åŠ©å®¢æ ˆ',
        r'çŸ¥è¯†é—®ç­”.*?å­—è¯è½¬æ¢.*?IRCå³æ—¶èŠå¤©.*?è”ç»œæˆ‘ä»¬',
        r'å…³äºç»´åŸºç™¾ç§‘.*?ç‰¹æ®Šé¡µé¢.*?æœç´¢.*?å¤–è§‚.*?èµ„åŠ©ç»´åŸºç™¾ç§‘',
        r'åˆ›å»ºè´¦å·.*?ç™»å½•.*?ä¸ªäººå·¥å…·.*?ç›®å½•.*?åºè¨€.*?å¼€å…³',
        r'å­ç« èŠ‚.*?æ³¨é‡Š.*?å»¶ä¼¸é˜…è¯».*?å‚è€ƒæ–‡çŒ®.*?æ¥æº.*?å‚è§',
        r'ç¼–è¾‘é“¾æ¥.*?æ¡ç›®è®¨è®º.*?ç®€ä½“.*?ä¸è½¬æ¢.*?ç¹ä½“',
        r'å¤§é™†ç®€ä½“.*?é¦™æ¸¯ç¹ä½“.*?æ¾³é–€ç¹é«”.*?å¤§é©¬ç®€ä½“.*?æ–°åŠ å¡ç®€ä½“.*?è‡ºç£æ­£é«”',
        r'é˜…è¯».*?ç¼–è¾‘.*?æŸ¥çœ‹å†å².*?å·¥å…·.*?æ“ä½œ.*?å¸¸è§„',
        r'é“¾å…¥é¡µé¢.*?ç›¸å…³æ›´æ”¹.*?ä¸Šä¼ æ–‡ä»¶.*?å›ºå®šé“¾æ¥.*?é¡µé¢ä¿¡æ¯',
        r'å¼•ç”¨æ­¤é¡µ.*?è·å–çŸ­é“¾æ¥.*?ä¸‹è½½äºŒç»´ç .*?æ‰“å°.*?å¯¼å‡º',
        r'ä¸‹è½½ä¸ºPDF.*?æ‰“å°ç‰ˆæœ¬.*?åœ¨å…¶ä»–é¡¹ç›®ä¸­.*?ç»´åŸºå…±äº«èµ„æº.*?ç»´åŸºæ•°æ®é¡¹ç›®',
        r'æ”¶è—.*?æŸ¥çœ‹æˆ‘çš„æ”¶è—.*?æœ‰ç”¨.*?åˆ†äº«.*?è¯„è®º.*?ç‚¹èµ',
        r'ç›¸å…³æ¨è.*?çƒ­é—¨.*?æœ€æ–°.*?æ›´å¤š.*?åŠ è½½æ›´å¤š',
        r'å¹¿å‘Š.*?æ¨å¹¿.*?èµåŠ©.*?å•†ä¸šåˆä½œ',
        r'ç‰ˆæƒ.*?å…è´£å£°æ˜.*?éšç§æ”¿ç­–.*?ä½¿ç”¨æ¡æ¬¾',
        r'è”ç³»æˆ‘ä»¬.*?å…³äºæˆ‘ä»¬.*?ç½‘ç«™åœ°å›¾.*?å¸®åŠ©ä¸­å¿ƒ',
        r'å†…å®¹.*?ä¼è¨€.*?on.*?å­—ç¬¦.*?è¯æ•°.*?ç±»å‹',
        r'ç»´åŸºç™¾ç§‘.*?è‡ªç”±çš„ç™¾ç§‘å…¨ä¹¦',
        r'ç¹ä½“å­—.*?ç®€åŒ–å­—.*?æ ‡éŸ³.*?å®˜è¯.*?ç°ä»£æ ‡å‡†æ±‰è¯­',
        r'å³å°†å†›.*?å‡èŠ‚.*?æ•…å®«.*?å—è–°æ®¿.*?ç”»åƒ.*?è½¦éª‘å°†å†›.*?å¸éš¶æ ¡å°‰.*?è¥¿ä¹¡ä¾¯',
        r'å›½å®¶.*?æ—¶ä»£.*?ä¸»å›.*?å§“.*?å§“å.*?å­—.*?å°çˆµ.*?å°åœ°.*?ç±è´¯.*?å‡ºç”Ÿ.*?é€ä¸–.*?è°¥å·.*?å¢“è‘¬.*?äº²å±',
        r'æ–°äº­ä¾¯.*?è¥¿ä¹¡å¿.*?å¹½å·.*?æ¶¿éƒ¡.*?æ²³åŒ—çœ.*?æ¶¿å·å¸‚.*?èœ€æ±‰.*?æ˜­çƒˆå¸.*?ç« æ­¦.*?ç›Šå·.*?å·´è¥¿éƒ¡.*?é˜†ä¸­å¿.*?å››å·çœ.*?é˜†ä¸­å¸‚',
        r'æ¡“ä¾¯.*?çµåº”ç‹.*?å‰èœ€.*?åŠ å°.*?æ­¦ä¹‰.*?å¿ æ˜¾.*?è‹±çƒˆ.*?çµæƒ .*?åŠ©é¡ºç‹.*?å…ƒæœ',
        r'å¼ æ¡“ä¾¯åº™.*?å¤´é¢….*?å¼ æ¡“ä¾¯ç¥ .*?èº«ä½“.*?å¦».*?å¤ä¾¯æ°.*?å­.*?å¼ è‹.*?å¼ ç».*?å¥³.*?æ•¬å“€çš‡å.*?å¼ çš‡å.*?å­™.*?å¼ éµ'
    ]
    
    for pattern in useless_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # 3. ç§»é™¤æ‹¼éŸ³å’ŒéŸ³æ ‡ä¿¡æ¯
    # ç§»é™¤æ±‰è¯­æ‹¼éŸ³ï¼ˆå¸¦å£°è°ƒï¼‰
    text = re.sub(r'[a-zA-ZÄÃ¡ÇÃ ÅÃ³Ç’Ã²Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœÅ„ÅˆÇ¹á¸¿]+(?:\s+[a-zA-ZÄÃ¡ÇÃ ÅÃ³Ç’Ã²Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœÅ„ÅˆÇ¹á¸¿]+)*\s*[0-9]*', '', text)
    
    # ç§»é™¤å›½é™…éŸ³æ ‡
    text = re.sub(r'\[[^\]]*\]', '', text)
    
    # ç§»é™¤å¨å¦¥ç›æ‹¼éŸ³ç­‰
    text = re.sub(r'[A-Z][a-z]+[0-9]\s+[A-Z][a-z]+[0-9]', '', text)
    
    # ç§»é™¤å„ç§æ‹¼éŸ³ç³»ç»Ÿ
    pinyin_systems = [
        r'å¨å¦¥ç‘ªæ‹¼éŸ³', r'å¨å¦¥ç›æ‹¼éŸ³', r'å¨å¦¥ç›å¼æ‹¼éŸ³',
        r'å›½é™…éŸ³æ ‡', r'IPA',
        r'é—½å—è¯­ç™½è¯å­—', r'å°è¯­ç½—é©¬å­—', r'ç²¤æ‹¼', r'è€¶é²æ‹¼éŸ³',
        r'æ±‰è¯­æ‹¼éŸ³', r'Hanyu Pinyin', r'Wade-Giles',
        r'æ³¨éŸ³ç¬¦å·', r'æ³¨éŸ³', r'æ ‡éŸ³', r'å‘éŸ³', r'è¯»éŸ³',
        r'å®˜è¯', r'æ–¹è¨€', r'é—½è¯­', r'ç²¤è¯­', r'å°è¯­',
        r'ç™½è¯å­—', r'ç½—é©¬å­—', r'æ‹¼éŸ³', r'éŸ³æ ‡'
    ]
    
    for pattern in pinyin_systems:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ç§»é™¤åŒ…å«æ‹¼éŸ³çš„è¡Œ
    lines = text.split('\n')
    filtered_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡æ‹¼éŸ³å†…å®¹
        pinyin_indicators = [
            r'[a-zA-ZÄÃ¡ÇÃ ÅÃ³Ç’Ã²Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœÅ„ÅˆÇ¹á¸¿]{2,}',  # è¿ç»­æ‹¼éŸ³å­—ç¬¦
            r'[A-Z][a-z]+[0-9]',  # å¨å¦¥ç›æ‹¼éŸ³
            r'\[[^\]]*[a-zA-Z][^\]]*\]',  # åŒ…å«å­—æ¯çš„éŸ³æ ‡
            r'[ä¸€-é¾¯]+\s*[a-zA-ZÄÃ¡ÇÃ ÅÃ³Ç’Ã²Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœÅ„ÅˆÇ¹á¸¿]+',  # ä¸­æ–‡+æ‹¼éŸ³
            r'[a-zA-ZÄÃ¡ÇÃ ÅÃ³Ç’Ã²Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœÅ„ÅˆÇ¹á¸¿]+\s*[ä¸€-é¾¯]+',  # æ‹¼éŸ³+ä¸­æ–‡
        ]
        
        is_pinyin_line = False
        for pattern in pinyin_indicators:
            if re.search(pattern, line):
                # è®¡ç®—æ‹¼éŸ³å†…å®¹çš„æ¯”ä¾‹
                pinyin_chars = len(re.findall(r'[a-zA-ZÄÃ¡ÇÃ ÅÃ³Ç’Ã²Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœÅ„ÅˆÇ¹á¸¿]', line))
                total_chars = len(line)
                if pinyin_chars > total_chars * 0.3:  # å¦‚æœæ‹¼éŸ³å­—ç¬¦è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯æ‹¼éŸ³è¡Œ
                    is_pinyin_line = True
                    break
        
        if not is_pinyin_line:
            filtered_lines.append(line)
    
    text = '\n'.join(filtered_lines)
    
    # 4. ç§»é™¤é‡å¤å†…å®¹
    lines = text.split('\n')
    unique_lines = []
    seen_lines = set()
    
    for line in lines:
        line = line.strip()
        if line and len(line) > 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„è¡Œ
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé¿å…é‡å¤å†…å®¹
            is_duplicate = False
            for seen_line in seen_lines:
                if _calculate_similarity(line, seen_line) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_lines.append(line)
                seen_lines.add(line)
    
    # 5. é‡æ–°ç»„åˆæ–‡æœ¬
    text = '\n'.join(unique_lines)
    
    # 6. ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 7. é™åˆ¶å†…å®¹é•¿åº¦
    if len(text) > 3000:
        # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å®Œæ•´çš„å¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        truncated_text = ""
        for sentence in sentences:
            if len(truncated_text + sentence) < 3000:
                truncated_text += sentence + "ã€‚"
            else:
                break
        text = truncated_text
    
    return text

def _calculate_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
    
    Args:
        text1: æ–‡æœ¬1
        text2: æ–‡æœ¬2
        
    Returns:
        ç›¸ä¼¼åº¦ (0-1)
    """
    if not text1 or not text2:
        return 0.0
    
    # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0.0