#!/usr/bin/env python3
"""
æµ‹è¯•é—²èŠåŠŸèƒ½ - éªŒè¯LLMåˆ¤æ–­
"""

import asyncio
import os
from core.smart_pipeline_engine import SmartPipelineEngine

async def test_chat_llm_only():
    """æµ‹è¯•é—²èŠåŠŸèƒ½ - éªŒè¯LLMåˆ¤æ–­"""
    
    # åˆå§‹åŒ–å¼•æ“ï¼Œå¼ºåˆ¶ä½¿ç”¨LLM
    engine = SmartPipelineEngine(
        use_llm=True,  # å¼ºåˆ¶ä½¿ç”¨LLM
        llm_config={
            "llm_model": "gpt-4o",
            "api_key": os.environ.get("OPENAI_API_KEY"),
            "api_base": os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
        }
    )
    
    # æµ‹è¯•å„ç§è¾“å…¥ç±»å‹
    test_inputs = [
        # é—²èŠç±»å‹
        "ä½ å¥½",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "ä½ æ˜¯è°ï¼Ÿ",
        "è°¢è°¢ä½ çš„å¸®åŠ©",
        "å†è§",
        "åœ¨å—ï¼Ÿ",
        
        # ä»»åŠ¡ç±»å‹
        "æœç´¢äººå·¥æ™ºèƒ½çš„æœ€æ–°å‘å±•",
        "å¸®æˆ‘æ—‹è½¬è¿™å¼ å›¾ç‰‡90åº¦",
        "ç¿»è¯‘è¿™æ®µè‹±æ–‡æ–‡æœ¬",
        "ç”Ÿæˆä¸€ä»½é¡¹ç›®æŠ¥å‘Š"
    ]
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•LLMé—²èŠåˆ¤æ–­åŠŸèƒ½...")
    print("=" * 60)
    
    for user_input in test_inputs:
        print(f"\nğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        
        try:
            result = await engine.execute_from_natural_language(user_input)
            
            if result["success"]:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é—²èŠ
                if result.get("final_output") and not result.get("node_results"):
                    print(f"ğŸ’¬ é—²èŠåˆ¤æ–­: {result['final_output']}")
                else:
                    print(f"ğŸ”§ ä»»åŠ¡æ‰§è¡Œ: æ‰§è¡Œäº† {len(result.get('node_results', []))} ä¸ªèŠ‚ç‚¹")
                    for node in result.get('node_results', []):
                        print(f"   - {node['tool_type']}: {node.get('status', 'unknown')}")
            else:
                print(f"âŒ å¤±è´¥: {result['errors']}")
                
        except Exception as e:
            print(f"âŒ å¼‚å¸¸: {e}")
    
    print("\nğŸ‰ LLMé—²èŠåˆ¤æ–­åŠŸèƒ½æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(test_chat_llm_only()) 